% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kld-estimation-nearest-neighbours.R
\name{kld_est_gnn}
\alias{kld_est_gnn}
\title{Generalized k-nearest neighbour KL divergence estimator.}
\usage{
kld_est_gnn(X, Y = NULL, q = NULL, l = k, k = 1, eps = 0.01, log.q = FALSE)
}
\arguments{
\item{X, Y}{\code{n}-by-\code{d} and \code{m}-by-\code{d} matrices, representing \code{n} samples from
the true distribution \eqn{P} and \code{m} samples from the approximate distribution
\eqn{Q}, both in \code{d} dimensions. Vector input is treated as a column matrix.
\code{Y} can be left blank if \code{q} is specified (see below).}

\item{q}{The density function of the approximate distribution \eqn{Q}. Either
\code{Y} or \code{q} must be specified.}

\item{l, k}{Scalars or numeric vectors of length \code{n} and \code{m}, representing the
number of nearest neighbours to use for nearest neighbour density estimation
of P and Q, respectively, for each of the data points \code{X[i,]}, with
\code{i} ranging from \code{1} to \code{n}, and \code{Y[j,]}, with
\code{j} ranging from \code{1} to \code{m}. The default is \code{l = k = 1}.
In the special case that \code{l = k} and \code{k} is scalar, the estimator coincides
with `kld_est_nn(X, Y, k).}

\item{eps}{Error bound in the nearest neighbour search. A value of \code{eps = 0}
implies exact nearest neighbour search, otherwise approximate nearest
neighbours are sought. Defaults to \code{eps = 0.01}.}

\item{log.q}{If \code{TRUE}, function \code{q} is the log-density rather than the density
of the approximate distribution \eqn{Q} (default: \code{log.q = FALSE}).}
}
\value{
A scalar, the estimated Kullback-Leibler divergence \eqn{D_{KL}(P||Q)}.
}
\description{
This function implements the generalized k-nearest neighbour estimator in
Wang et al. (2009), Eq.(17). In this estimator, the number of nearest
neighbours to consider may differ between samples and sample points.
Currently, this estimator is only implemented for the two-sample problem.
}
\details{
Reference:
Wang, Kulkarni and Verd√∫, "Divergence Estimation for Multidimensional
Densities Via k-Nearest-Neighbor Distances", IEEE Transactions on Information
Theory, Vol. 55, No. 5 (2009).
}
