% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kld-estimation-continuous.R
\name{kld_est_knn}
\alias{kld_est_knn}
\title{Generalized k-nearest neighbour KL divergence estimator by Wang et al. (2009)
Equation (17)}
\usage{
kld_est_knn(X, Y, l = k, k = 1)
}
\arguments{
\item{X, Y}{\code{n}-by-\code{d} and \code{m}-by-\code{d} matrices, representing \code{n} samples from
the true distribution \eqn{P} and \code{m} samples from the approximate distribution
\eqn{Q}, both in \code{d} dimensions. Vector input is treated as a column matrix.}

\item{l, k}{Scalars or numeric vectors of length \code{n} and \code{m}, representing the
number of nearest neighbours to use for nearest neighbour density estimation
of P and Q, respectively, for each of the data points \code{X[i,]}, with
\code{i} ranging from \code{1} to \code{n}, and \code{Y[j,]}, with
\code{j} ranging from \code{1} to \code{m}. The default is \code{l = k = 1}.}
}
\value{
A scalar, the estimated Kullback-Leibler divergence D(P||Q).
}
\description{
Reference:
Wang, Kulkarni and Verd√∫, "Divergence Estimation for Multidimensional
Densities Via k-Nearest-Neighbor Distances", IEEE Transactions on Information
Theory, Vol. 55, No. 5 (2009).
}
\examples{
# KL-D between two samples from 1-D Gaussians:
X <- rnorm(100)
Y <- rnorm(100, mean = 1, sd = 2)
kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2)
kld_est_kde1(X, Y)
kld_est_1nn(X, Y)
kld_gknn(X, Y)

# KL-D between two samples from 2-D Gaussians:
X1 <- rnorm(100)
X2 <- rnorm(100)
Y1 <- rnorm(100)
Y2 <- Y1 + rnorm(100)
X <- cbind(X1,X2)
Y <- cbind(Y1,Y2)
kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),
             mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2))
kld_est_1nn(X, Y)
kld_est_gknn(X, Y)
}
