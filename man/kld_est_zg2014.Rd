% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kld-estimation-discrete.R
\name{kld_est_zg2014}
\alias{kld_est_zg2014}
\title{Zhang/Grabchak KL divergence estimator for samples from discrete distributions}
\usage{
kld_est_zg2014(X, Y)
}
\arguments{
\item{X, Y}{Two samples from discrete distributions, specified as vectors,
matrices or data frames.}
}
\value{
A scalar, the estimated Kullback-Leibler divergence \eqn{D_{KL}(P||Q)}.
}
\description{
CAVE: not implemented yet!
}
\details{
The estimator is that from Eq. (1.3) from Zhang and Grabchak (2014).

Reference:
Zhang and Grabchak, "Nonparametric Estimation of Kullback-Leibler Divergence",
Neural Computation 26, 2570â€“2593 (2014).
}
\examples{
# 1D example
X <- c(rep('M',5),rep('F',5))
Y <- c(rep('M',6),rep('F',4))
kld_est_discrete(X, Y)
}
