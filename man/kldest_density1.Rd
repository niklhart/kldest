% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kld-estimation.R
\name{kldest_density1}
\alias{kldest_density1}
\title{1-D density-based estimation of Kullback-Leibler divergence}
\usage{
kldest_density1(X, Y)
}
\arguments{
\item{X, Y}{Numeric vectors or single-column matrices, representing samples
from the true distribution \eqn{P} and the approximate distribution
\eqn{Q}, respectively.}
}
\value{
A scalar, the estimated Kullback-Leibler divergence \eqn{D_{KL}(P||Q)}.
}
\description{
This estimation method approximates the densities of the unknown distributions
\eqn{P} and \eqn{Q} by a kernel density estimate using function 'density'.
}
\examples{
# KL-D between two samples from 1D Gaussians:
X <- rnorm(100)
Y <- rnorm(100, mean = 1, sd = 2)
kl_div_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2)
kldest_density1(X,Y)
}
