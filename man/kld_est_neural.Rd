% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kld-estimation-neural.R
\name{kld_est_neural}
\alias{kld_est_neural}
\title{Neural KL divergence estimation (Donsker-Varadhan representation) using \code{torch}}
\usage{
kld_est_neural(
  X,
  Y,
  d_hidden = 1024,
  learning_rate = 1e-04,
  epochs = 5000,
  device = c("cpu", "cuda", "mps"),
  verbose = FALSE
)
}
\arguments{
\item{X, Y}{\code{n}-by-\code{d} and \code{m}-by-\code{d} numeric matrices, representing \code{n}
samples from the true distribution \eqn{P} and \code{m} samples from the
approximate distribution \eqn{Q}, both in \code{d} dimensions. Vector input is
treated as a column matrix.}

\item{d_hidden}{Number of nodes in hidden layer (default: \code{32})}

\item{learning_rate}{Learning rate during gradient descent (default: \code{1e-4})}

\item{epochs}{Number of training epochs (default: \code{200})}

\item{device}{Calculation device, either \code{"cpu"} (default), \code{"cuda"} or \code{"mps"}.}

\item{verbose}{Generate progress report to consolue during training of the
neutral network (default: \code{FALSE})?}
}
\value{
A scalar, the estimated Kullback-Leibler divergence \eqn{\hat D_{KL}(P||Q)}.
}
\description{
Estimation of KL divergence between continuous distributions based on the
Donsker-Varadhan representation
\deqn{D_{KL}(P||Q) = \sup_{f} E_P[f(X)] - \log\left(E_Q[e^{f(X)}]\right)}
using Monte Carlo averages to approximate the expectations, and optimizing
over a class of neural networks. The \code{torch} package is required to use this
function.
}
\details{
Disclaimer: this is a simple test implementation which is not optimized by
any means. In particular:
\itemize{
\item it uses a fully connected network with (only) a single hidden layer
\item it uses standard gradient descient on the full dataset and not more advanced
estimators
Also, he syntax is likely to change in the future.
}

Estimation is done as described for mutual information in Belghazi et al.
(see ref. below), except that standard gradient descent is used on the full
samples X and Y instead of using batches. Indeed, in the case where X and Y
have a different length, batch sampling is not that straightforward.

Reference: Belghazi et al., Mutual Information Neural Estimation,
PMLR 80:531-540, 2018.
}
\examples{
# 2D example
# analytical solution
kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),
             mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2))
# sample generation
set.seed(0)
nxy <- 1000
X1 <- rnorm(nxy)
X2 <- rnorm(nxy)
Y1 <- rnorm(nxy)
Y2 <- Y1 + rnorm(nxy)
X <- cbind(X1,X2)
Y <- cbind(Y1,Y2)
# Estimation
kld_est_nn(X, Y)
\dontrun{
# requires the torch package and takes ~1 min
kld_est_neural(X, Y)
}
}
