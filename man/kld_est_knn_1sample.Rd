% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kld-estimation-single-sample.R
\name{kld_est_knn_1sample}
\alias{kld_est_knn_1sample}
\title{Single-sample version of the k-NN divergence estimator from Wang et al. (2009).}
\usage{
kld_est_knn_1sample(X, q, k = 1L)
}
\arguments{
\item{X}{An \code{n}-by-\code{d} matrix, representing \code{n} samples from the true
distribution \eqn{P} in \code{d} dimensions. Vector input is treated as a
column matrix.}

\item{q}{The density of the approximate model \eqn{Q}.}

\item{k}{The number of neighbours to consider (default: \code{k=1}).}
}
\value{
A scalar, the estimated Kullback-Leibler divergence \eqn{D_{KL}(P||Q)}.
}
\description{
This function experiments with partial nearest-neighbour estimation, i.e.
only for p, but not for q, which may be available analytically (the estimated
approximate model). For now, it really doesn't work well.
}
\examples{
X <- rnorm(100)
Y <- rnorm(100, mean = 1, sd = 2)
q <- function(x) dnorm(x, mean = 1, sd = 2)
kld_gaussian(0,1,1,2^2)
kld_est_1nn(X, Y)
kld_est_knn_1sample(X, q, k = 10)
}
