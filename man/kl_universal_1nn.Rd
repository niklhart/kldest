% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kld-estimation.R
\name{kl_universal_1nn}
\alias{kl_universal_1nn}
\title{Universal 1-nearest neighbour divergence estimator from Wang et al. (2009).}
\usage{
kl_universal_1nn(X, Y)
}
\arguments{
\item{X, Y}{\code{n}-by-\code{d} and \code{m}-by-\code{d} matrices, representing \code{n} samples from
the true distribution P and \code{m} samples from the approximate distribution
Q, both in \code{d} dimensions. Vector input is treated as a column matrix.}
}
\value{
A scalar, the estimated Kullback-Leibler divergence \eqn{D_{KL}(P||Q)}.
}
\description{
Direct implementation of the universal 1-NN divergence estimator from eq. (5)
in Wang et al. (2009). The estimator is identical to the one by Perez-Cruz
(2008).
}
\details{
Code source: https://gist.github.com/lars-von-buchholtz/636f542ce8d93d5a14ae52a6c538ced5
Adapted for R from python code at https://gist.github.com/atabakd/ed0f7581f8510c8587bc2f41a094b518
}
\examples{
# KL-D between two samples from 1D Gaussians:
X <- rnorm(1000)
Y <- rnorm(1000, mean = 1, sd = 2)
kl_divergence_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2)
kldest_density(X, Y)
kl_universal_1nn(X, Y)
kl_generalized_knn_eps(X, Y)
}
