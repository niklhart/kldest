% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kld-uncertainty.R
\name{kld_ci_subboot}
\alias{kld_ci_subboot}
\title{Uncertainty of KL divergence estimate using the subsampling bootstrap.}
\usage{
kld_ci_subboot(
  X,
  Y,
  estimator = kld_est_1nn,
  B = 100L,
  alpha = 0.05,
  size = function(x) x^(2/3),
  rate = sqrt
)
}
\arguments{
\item{X, Y}{\code{n}-by-\code{d} and \code{m}-by-\code{d} matrices, representing \code{n} samples from
the true distribution \eqn{P} and \code{m} samples from the approximate distribution
\eqn{Q}, both in \code{d} dimensions. Vector input is treated as a column matrix.}

\item{estimator}{A function handle expecting two inputs \code{X} and \code{Y}, the
Kullback-Leibler divergence estimation method. Defaults to \code{kld_est_1nn}.}

\item{B}{Number of bootstrap replicates (default: \code{100}), the larger, the
more accurate, but also more computationally expensive.}

\item{alpha}{Error level, defaults to \code{0.05}.}

\item{size}{A function specifying the size of the subsamples, defaults to
\eqn{f(x) = x^{2/3}}.}

\item{rate}{A function computing the convergence rate of the estimator as a
function of sample sizes. Defaults to \code{sqrt}.}
}
\value{
A list with the fields \code{"kld"} (the estimated KL divergence),
\code{"boot"} (a length \code{B} numeric vector with KL divergence estimates on
the bootstrap subsamples), and \code{"ci"} (a length \code{2} vector containing the
lower and upper limits of the estimated confidence interval).
}
\description{
This function computes a confidence interval for KL divergence based on the
subsampling bootstrap by Politis and Romano. The calculated interval has
asymptotic coverage \eqn{1 - \alpha} as long as \eqn{b_n/n\rightarrow 0},
\eqn{b_n\rightarrow\infty} and \eqn{\frac{\tau_b}{\tau_n}\rightarrow 0}.
}
\details{
Reference:
Politis and Romano, "Large sample confidence regions based on subsamples under
minimal assumptions", The Annals of Statistics, Vol. 22, No. 4 (1994).
}
\examples{
# 1D Gaussian
X <- rnorm(100)
Y <- rnorm(100, mean = 1, sd = 2)
kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2)
kld_est_1nn(X, Y)
kld_ci_subboot(X, Y)

}
