% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kld-estimation.R
\name{kldest_density}
\alias{kldest_density}
\title{Density-based estimation of Kullback-Leibler divergence}
\usage{
kldest_density(X, Y, h = NULL, K = dnorm)
}
\arguments{
\item{X}{A numeric vector or matrix, representing the sample from the true
distribution.}

\item{Y}{A numeric vector or matrix, representing the sample from the true
distribution. If \code{X} and \code{Y} matrices, they must have the same
number of columns \code{d}.}

\item{h}{A positive scalar or length \code{d} vector, representing the bandwidth
parameter (possibly different in each component). The default \code{NULL} uses
Scott's rule (EMBED LATEX?)}

\item{K}{A density function representing the kernel to be used. Defaults to a Gaussian kernel.}
}
\value{
A scalar, the estimated Kullback-Leibler divergence
}
\description{
This estimation method approximates the unknown densities p and q by a kernel
density estimate, using a sample size- and dimension-dependent bandwidth parameter
and a Gaussian kernel.
}
\examples{
# KL-D between two samples from 1D Gaussians:
X <- rnorm(100)
Y <- rnorm(100, mean = 1, sd = 2)
kl_divergence_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2)
kldest_density(X,Y)
}
