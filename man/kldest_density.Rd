% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kld-estimation.R
\name{kldest_density}
\alias{kldest_density}
\title{Density-based estimation of Kullback-Leibler divergence}
\usage{
kldest_density(X, Y, hX = NULL, hY = NULL)
}
\arguments{
\item{X, Y}{\code{n}-by-\code{d} and \code{m}-by-\code{d} matrices, representing \code{n} samples from
the true distribution P and \code{m} samples from the approximate distribution
Q, both in \code{d} dimensions. Vector input is treated as a column matrix.}

\item{hX, hY}{Positive scalars or length \code{d} vectors, representing bandwidth
parameters (possibly different in each component) for the density estimates
of P and Q, respectively. The default \code{NULL} uses Scott's rule,
\deqn{h_i = \frac{\sigma_i}{n^{1/(d+4)}}}}
}
\value{
A scalar, the estimated Kullback-Leibler divergence \eqn{D_{KL}(P||Q)}.
}
\description{
This estimation method approximates the unknown densities P and Q by a kernel
density estimate, using a sample size- and dimension-dependent bandwidth parameter
and a Gaussian kernel.
}
\examples{
# KL-D between two samples from 1D Gaussians:
X <- rnorm(100)
Y <- rnorm(100, mean = 1, sd = 2)
kl_divergence_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2)
kldest_density(X,Y)
}
