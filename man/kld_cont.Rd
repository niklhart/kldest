% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kld-estimation-interfaces.R
\name{kld_cont}
\alias{kld_cont}
\title{Kullback-Leibler divergence estimation between continuous distributions}
\usage{
kld_cont(..., method = c("nn", "gnn", "brnn", "kde1", "kde2"))
}
\arguments{
\item{...}{Arguments to be passed to the respective method (see links below).}

\item{method}{The estimation method to be used, either \code{nn} for nearest-neighbour
based estimation (the default), \code{gnn} for generalized nearest neightbour
estimation, \code{brnn} for bias-reduced generalized nearest neighbour estimation,
\code{kde1} for kernel density based estimation in 1D or \code{kde2} for kernel density
based estimation in 2D. The last option requires package \code{KernSmooth}
to be installed.}
}
\value{
A scalar, the estimated Kullback-Leibler divergence \eqn{D_{KL}(P||Q)}.
}
\description{
This function is an interface to several different estimation methods for
Kullback-Leibler divergence \eqn{D_{KL}(P||Q)} between two continuous
distributions \eqn{P} and \eqn{Q} based on a sample \code{X} from \eqn{P} and
either the density function \code{q} of \eqn{Q} (one-sample problem) or a sample
\code{Y} of \eqn{Q} (two-sample problem).
}
\examples{
# KL-D between two samples from 1-D Gaussians:
X <- rnorm(100)
Y <- rnorm(100, mean = 1, sd = 2)
kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2)
kld_est_kde1(X, Y)
kld_est_1nn(X, Y)
kld_gknn(X, Y)

# KL-D between two samples from 2-D Gaussians:
X1 <- rnorm(100)
X2 <- rnorm(100)
Y1 <- rnorm(100)
Y2 <- Y1 + rnorm(100)
X <- cbind(X1,X2)
Y <- cbind(Y1,Y2)
kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),
             mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2))
kld_est_1nn(X, Y)
kld_est_gknn(X, Y)
}
\seealso{
\code{\link[=kld_est_nn]{kld_est_nn()}}, \code{\link[=kld_est_gnn]{kld_est_gnn()}}, \code{\link[=kld_est_brnn]{kld_est_brnn()}}, \code{\link[=kld_est_kde1]{kld_est_kde1()}},
\code{\link[=kld_est_kde2]{kld_est_kde2()}} for
}
