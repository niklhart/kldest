% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kld-estimation.R
\name{kl_generalized_knn_eps}
\alias{kl_generalized_knn_eps}
\title{Generalized KL divergence estimation method from Wang et al. (2009)}
\usage{
kl_generalized_knn_eps(X, Y, max.k = 50, warn.max.k = TRUE)
}
\arguments{
\item{X, Y}{\code{n}-by-\code{d} and \code{m}-by-\code{d} matrices, representing \code{n} samples from
the true distribution \eqn{P} and \code{m} samples from the approximate distribution
\eqn{Q}, both in \code{d} dimensions. Vector input is treated as a column matrix.}

\item{max.k}{Maximum numbers of nearest neighbours to compute (default: \code{50})}

\item{warn.max.k}{If \code{TRUE} (the default), warn if \code{max.k} is such that more
than \code{max.k} neighbours are within the \code{eps} neighbourhood for some data
points. In this case, only the first \code{max.k} neighbours will be counted.
As a consequence, \code{max.k} may required to be increased.}
}
\value{
A scalar, the estimated Kullback-Leibler divergence \eqn{D_{KL}(P||Q)}.
}
\description{
This is the generalized k-NN based KL divergence estimator, eq. (29).
\code{TODO}: more details on the algorithm!
}
\details{
Reference: https://doi.org/10.1109/TIT.2009.2016060
}
