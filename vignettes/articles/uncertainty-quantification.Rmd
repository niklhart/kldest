---
title: "Confidence intervals for KL divergence (1-D)"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(kldest)
library(ggplot2)
library(reshape2)
set.seed(0)
```

We investigate the following pairs of distributions,
for which analytical KL divergence values are known:

* $\text{Exp}(1)$ vs. $\text{Exp}(1/12)$,
* $\mathcal{N}(0,1)$ vs. $\mathcal{N}(1,2^2)$,
* $\mathcal{U}(1,2)$ vs. $\mathcal{U}(0,4)$.
* $\mathcal{U}(0,1)$ vs. $\mathcal{N}(0,1)$.

```{r}
p <- list(
    exponential = list(lambda1 = 1, lambda2 = 1/12),
    gaussian    = list(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2),
    uniform     = list(a1 = 1, b1 = 2, a2 = 0, b2 = 4),
    unif_gauss  = list(a = 0, b = 1, mu = 0, sigma2 = 1)
)
distributions <- list(
    exponential = list(
        samples = function(n, m) {
            X <- rexp(n, rate = p$exponential$lambda1)
            Y <- rexp(m, rate = p$exponential$lambda2)
            list(X = X, Y = Y)
        },
        kld = do.call(kld_exponential, p$exponential)
    ),
    gaussian = list(
        samples = function(n, m) {
            X <- rnorm(n, mean = p$gaussian$mu1, sd = sqrt(p$gaussian$sigma1))
            Y <- rnorm(m, mean = p$gaussian$mu2, sd = sqrt(p$gaussian$sigma2))
            list(X = X, Y = Y)
        },
        kld = do.call(kld_gaussian, p$gaussian)
    ),
    uniform = list( 
        samples = function(n, m) {
            X <- runif(n, min = p$uniform$a1, max = p$uniform$b1)
            Y <- runif(m, min = p$uniform$a2, max = p$uniform$b2)
            list(X = X, Y = Y)
        },
        kld = do.call(kld_uniform, p$uniform)
    ),
    unif_gauss = list(
        samples = function(n, m) {
            X <- runif(n, min = p$unif_gauss$a, max = p$unif_gauss$b)
            Y <- rnorm(m, mean = p$unif_gauss$mu, sd = sqrt(p$unif_gauss$sigma2))
            list(X = X, Y = Y)
        },
        kld = do.call(kld_uniform_gaussian, p$unif_gauss)
    )
)
```

Analytical values for Kullback-Leibler divergences in test cases:

```{r}
vapply(distributions, function(x) x$kld, 1)
```

### Simulation scenarios

For each of the distributions specified above, samples of different sizes are 
drawn, with several replicates per distribution and sample size.

```{r}
samplesize <- c(100,1000)#,10000)
nRep       <- 100L

scenarios <- combinations(
    distribution = names(distributions),
    sample.size  = samplesize,
    replicate    = 1:nRep
)
```

### Algorithm & bootstrap methods

The following estimator is considered:

```{r}
estimator <- kld_est_nn
```

We use the following subsampling variants:

```{r}
resampling <- list(
    sub_n12_tau12 = function(...) kld_ci_subsampling(..., subsample.size = function(n) n^(1/2)), 
    sub_n23_tau12 = function(...) kld_ci_subsampling(..., subsample.size = function(n) n^(2/3)), 
    sub_n12_tauest = function(...) kld_ci_subsampling(..., convergence.rate = NULL, subsample.size = function(n) n^(1/2)), 
    sub_n12se_tau12 = function(...) kld_ci_subsampling(..., subsample.size = function(n) n^(1/2), method = "se"),
    sub_n23se_tau12 = function(...) kld_ci_subsampling(..., subsample.size = function(n) n^(2/3), method = "se")
)
nResamp <- length(resampling)
```

## Uncertainty quantification of estimators

### Calculation of empirical coverage 

```{r}
# allocating results matrices
nscenario  <- nrow(scenarios)
covered <- matrix(nrow = nscenario, 
                  ncol = nResamp, 
                  dimnames = list(NULL, names(resampling)))

# looping over scenarios
for (i in 1:nscenario) {

    dist <- scenarios$distribution[i]
    n    <- scenarios$sample.size[i]
    
    samples <- distributions[[dist]]$sample(n = n, m = n)
    kld     <- distributions[[dist]]$kld
    
    X <- samples$X
    Y <- samples$Y
    
    # different algorithms are evaluated on the same samples
    for (j in 1:nResamp) {
        kldboot <- resampling[[j]](X, Y, estimator = estimator, B = 500L)
        covered[i,j] <- kldboot$ci[1] <= kld && kld <= kldboot$ci[2]
    }
}
```

Combine scenarios with CI coverage information:

```{r}
results <- cbind(scenarios, covered) |> melt(measure.vars  = names(resampling),
                                             value.name    = "covered",
                                             variable.name = "resampling") 
```

Compute coverage per sample size / algorithm / distribution:

```{r}
coverage <- dcast(results, 
                  sample.size + resampling + distribution ~ ., 
                  value.var = "covered", 
                  fun.aggregate = function(x) mean(x, na.rm = TRUE))
names(coverage)[4] <- "coverage"
```

### Results: coverage of confidence intervals for KL divergence

```{r}
ggplot(coverage, aes(x = sample.size, y = coverage, color = resampling)) + 
    facet_wrap("distribution") +
    geom_line() + 
    scale_x_log10() +
    geom_hline(yintercept = 0.95, lty = 2) + 
    scale_color_discrete(name = "CI method", 
                        labels = c("Reverse percentile, s = n^(1/2), tau = n^(1/2)",
                                   "Reverse percentile, s = n^(2/3), tau = n^(1/2)",
                                   "Reverse percentile, s = n^(1/2), tau estimated",
                                   "Reverse percentile, s = n^(2/3), tau estimated"#,
                                  # "1.96 standard error, s = n^(1/2), tau = n^(1/2)",
                                  # "1.96 standard error, s = n^(2/3), tau = n^(1/2)"
                                  )) +
    ggtitle("Subsampling-based confidence intervals") +
    theme(plot.title = element_text(hjust = 0.5))
```

$\Rightarrow$ coverage of the confidence intervals based on nearest neighbour 
density estimation approaches the nominal coverage of 95% for increasing sample 
sizes. 

For the Gaussian case, in which the kernel density based method performs
well in the estimation benchmark, this also holds for the kernel density method.
However, due to the bias in estimating KL divergence between exponential or
uniform distributions, uncertainty quantification also fails.

When using bootstrapping instead of subsampling, results are similar for kernel
density estimation, but change dramatically for nearest neighbour density 
estimation, which cannot deal with the ties produced by resampling with replacement.


