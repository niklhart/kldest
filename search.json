[{"path":"https://niklhart.github.io/kldest/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 kldest authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":[]},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-10d.html","id":"distributions-and-analytical-kl-divergence","dir":"Articles","previous_headings":"Specification of benchmark scenario","what":"Distributions and analytical KL divergence","title":"Algorithm benchmark in high dimensions","text":"Simulation study design Analytical values Kullback-Leibler divergences test cases:","code":"D <- 10 Sigma <- constDiagMatrix(dim = D, diag = 1, offDiag = 0.999) p <- list(     gaussian_10d = list(         paramTrue   = list(mu = rep(0, D), sigma = Sigma),         paramApprox = list(mu = rep(1, D), sigma = Sigma)     ) ) distributions <- list(     gaussian_10d = list(         samples = function(n, m) {             X <- mvrnorm(n = n,                           mu = p$gaussian_10d$paramTrue$mu,                             Sigma = p$gaussian_10d$paramTrue$sigma)             Y <- mvrnorm(n = m,                           mu = p$gaussian_10d$paramApprox$mu,                           Sigma = p$gaussian_10d$paramApprox$sigma)             list(X = X, Y = Y)         },         q = function(x) mvdnorm(x, mu = p$gaussian_10d$paramApprox$mu,                                  Sigma = p$gaussian_10d$paramApprox$sigma),         kld = kld_gaussian(             mu1    = p$gaussian_10d$paramTrue$mu,             sigma1 = p$gaussian_10d$paramTrue$sigma,             mu2    = p$gaussian_10d$paramApprox$mu,             sigma2 = p$gaussian_10d$paramApprox$sigma)     ) ) vapply(distributions, function(x) x$kld, 1) #> gaussian_10d  #>    0.5004504"},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-10d.html","id":"simulation-scenarios","dir":"Articles","previous_headings":"Specification of benchmark scenario","what":"Simulation scenarios","title":"Algorithm benchmark in high dimensions","text":"distributions specified , samples different sizes drawn, several replicates per distribution sample size.","code":"samplesize <- 10^(2:4) #n <- c(20,50,100,200,500,1000,2000,5000,10000) nRep       <- 25L  scenarios <- combinations(     distribution = names(distributions),     sample.size  = samplesize,     replicate    = 1:nRep )"},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-10d.html","id":"algorithms","dir":"Articles","previous_headings":"Specification of benchmark scenario","what":"Algorithms","title":"Algorithm benchmark in high dimensions","text":"following algorithms considered:","code":"algorithms_XY <- list(     nn1_XY   = function(X, Y) kld_est_nn(X, Y),     nn_br_XY = function(X, Y) kld_est_brnn(X, Y, warn.max.k = FALSE) ) algorithms_Xq <- list(     nn1_Xq = function(X, q) kld_est_nn(X, q = q) ) nAlgoXY  <- length(algorithms_XY) nAlgoXq  <- length(algorithms_Xq) nmAlgo   <- c(names(algorithms_XY),names(algorithms_Xq))"},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-10d.html","id":"run-the-simulation-study","dir":"Articles","previous_headings":"","what":"Run the simulation study","title":"Algorithm benchmark in high dimensions","text":"Post-processing: combine scenarios, kldiv runtime single data frame","code":"# allocating results matrices nscenario  <- nrow(scenarios) runtime <- kld <- matrix(nrow = nscenario,                           ncol = nAlgoXY+nAlgoXq,                           dimnames = list(NULL, nmAlgo))  for (i in 1:nscenario) {      dist <- scenarios$distribution[i]     n    <- scenarios$sample.size[i]          samples <- distributions[[dist]]$sample(n = n, m = n)     X <- samples$X     Y <- samples$Y     q <- distributions[[dist]]$q      # different algorithms are evaluated on the same samples     for (j in 1:nAlgoXY) {         algo         <- algorithms_XY[[j]]         start_time   <- Sys.time()         kld[i,j]     <- algo(X, Y)         end_time     <- Sys.time()         runtime[i,j] <- end_time - start_time     }     for (j in 1:nAlgoXq) {         nj            <- nAlgoXY+j         algo          <- algorithms_Xq[[j]]         start_time    <- Sys.time()         kld[i,nj]     <- algo(X, q)         end_time      <- Sys.time()         runtime[i,nj] <- end_time - start_time     } } tmp1 <- cbind(scenarios, kld) |> melt(measure.vars  = nmAlgo,                                       value.name    = \"kld\",                                       variable.name = \"algorithm\")  tmp2 <- cbind(scenarios, runtime) |> melt(measure.vars  = nmAlgo,                                           value.name    = \"runtime\",                                           variable.name = \"algorithm\")  results <- merge(tmp1,tmp2) results$sample.size <- as.factor(results$sample.size) rm(tmp1,tmp2)"},{"path":[]},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-10d.html","id":"accuracy-of-kl-divergence-estimators","dir":"Articles","previous_headings":"Graphical display of results","what":"Accuracy of KL divergence estimators","title":"Algorithm benchmark in high dimensions","text":"\\(\\Rightarrow\\) bias-reduced nearest neighbour algorithm shows much better performance either plain 1-nearest neighbour algorithms. particular, even approximate density q known (e.g., model fitted data X), may preferable simulate (large) sample Y \\(Q\\) use bias-reduced algorithm two-sample problem, rather using q.","code":"ggplot(results, aes(x=sample.size, y=kld, color=algorithm)) +      geom_jitter(position=position_dodge(0.5)) +      facet_wrap(\"distribution\", scales = \"free_y\") +     geom_hline(data = data.frame(distribution = names(distributions),                                   kldtrue = vapply(distributions, function(x) x$kld,1)),                 aes(yintercept = kldtrue)) +     xlab(\"Sample sizes\") + ylab(\"KL divergence estimate\") +      ggtitle(\"Accuracy of different algorithms\") +     theme(plot.title = element_text(hjust = 0.5))"},{"path":[]},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-1d.html","id":"distributions-and-analytical-kl-divergence","dir":"Articles","previous_headings":"Specification of benchmark scenario","what":"Distributions and analytical KL divergence","title":"Algorithm benchmark in 1D","text":"investigate following pairs distributions, analytical KL divergence values known: \\(\\text{Exp}(1)\\) vs. \\(\\text{Exp}(1/12)\\). \\(\\mathcal{N}(0,1)\\) vs. \\(\\mathcal{N}(1,2^2)\\), \\(\\mathcal{U}(1,2)\\) vs. \\(\\mathcal{U}(0,4)\\), Analytical values Kullback-Leibler divergences test cases:","code":"p <- list(     exponential = list(lambda1 = 1, lambda2 = 1/12),     gaussian    = list(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2),     uniform     = list(a1 = 1, b1 = 2, a2 = 0, b2 = 4) ) distributions <- list(     exponential = list(         samples = function(n, m) {             X <- rexp(n, rate = p$exponential$lambda1)             Y <- rexp(m, rate = p$exponential$lambda2)             list(X = X, Y = Y)         },         kld = do.call(kld_exponential, p$exponential)     ),     gaussian = list(         samples = function(n, m) {             X <- rnorm(n, mean = p$gaussian$mu1, sd = sqrt(p$gaussian$sigma1))             Y <- rnorm(m, mean = p$gaussian$mu2, sd = sqrt(p$gaussian$sigma2))             list(X = X, Y = Y)         },         kld = do.call(kld_gaussian, p$gaussian)     ),     uniform = list(          samples = function(n, m) {             X <- runif(n, min = p$uniform$a1, max = p$uniform$b1)             Y <- runif(m, min = p$uniform$a2, max = p$uniform$b2)             list(X = X, Y = Y)         },         kld = do.call(kld_uniform, p$uniform)     ) ) vapply(distributions, function(x) x$kld, 1) #> exponential    gaussian     uniform  #>   1.5682400   0.4431472   1.3862944"},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-1d.html","id":"simulation-scenarios","dir":"Articles","previous_headings":"Specification of benchmark scenario","what":"Simulation scenarios","title":"Algorithm benchmark in 1D","text":"distributions specified , samples different sizes drawn, several replicates per distribution sample size.","code":"samplesize <- 10^(2:4) nRep       <- 25L  scenarios <- combinations(     distribution = names(distributions),     sample.size  = samplesize,     replicate    = 1:nRep )"},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-1d.html","id":"algorithms","dir":"Articles","previous_headings":"Specification of benchmark scenario","what":"Algorithms","title":"Algorithm benchmark in 1D","text":"consder following algorithms: kernel density estimation numerical integration (dens_int) kernel density estimation Monte Carlo approximation (dens_mc) 1-nearest neighbour density estimation (nn_1) bias-reduced nearest neighbour density estimation (nn_br)","code":"algorithms <- list(     dens_int = function(X, Y) kld_est_kde1(X = X, Y = Y, MC = FALSE),     dens_mc  = function(X, Y) kld_est_kde1(X = X, Y = Y, MC = TRUE),     nn_1     = kld_est_nn,     nn_br = function(X, Y) kld_est_brnn(X = X, Y = Y, warn.max.k = FALSE) ) nAlgo   <- length(algorithms)"},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-1d.html","id":"run-the-simulation-study","dir":"Articles","previous_headings":"","what":"Run the simulation study","title":"Algorithm benchmark in 1D","text":"Post-processing: combine scenarios, kldiv1d runtime single data frame","code":"# allocating results matrices nscenario  <- nrow(scenarios) runtime <- kldiv1d <- matrix(nrow = nscenario,                               ncol = nAlgo,                               dimnames = list(NULL, names(algorithms)))  for (i in 1:nscenario) {      dist <- scenarios$distribution[i]     n    <- scenarios$sample.size[i]          samples <- distributions[[dist]]$sample(n = n, m = n)     X <- samples$X     Y <- samples$Y          # different algorithms are evaluated on the same samples     for (j in 1:nAlgo) {         algo <- algorithms[[j]]         start_time <- Sys.time()         kldiv1d[i,j] <- algo(X, Y)         end_time <- Sys.time()         runtime[i,j] <- end_time - start_time     } } tmp1 <- cbind(scenarios, kldiv1d) |> melt(measure.vars = names(algorithms),                                           value.name = \"kld\",                                           variable.name = \"algorithm\")  tmp2 <- cbind(scenarios, runtime) |> melt(measure.vars = names(algorithms),                                           value.name = \"runtime\",                                           variable.name = \"algorithm\")  results <- merge(tmp1,tmp2) results$sample.size <- as.factor(results$sample.size) rm(tmp1,tmp2)"},{"path":[]},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-1d.html","id":"accuracy-of-kl-divergence-estimators","dir":"Articles","previous_headings":"Results","what":"Accuracy of KL divergence estimators","title":"Algorithm benchmark in 1D","text":"\\(\\Rightarrow\\) estimators converge towards true KL divergence (black solid line). Kernel density-based estimators generally lower variance nearest neighbour-based estimators, show finite sample bias, especially asymmetric exponential distribution. difference 1-nearest neighbour bias-reduced k-nearest neighbour methods terms accuracy.","code":"ggplot(results, aes(x=sample.size, y=kld, color=algorithm)) +      geom_jitter(position=position_dodge(.5)) +      facet_wrap(\"distribution\", scales = \"free_y\") +     geom_hline(data = data.frame(distribution = names(distributions),                                   kldtrue = vapply(distributions, function(x) x$kld,1)),                 aes(yintercept = kldtrue)) +     xlab(\"Sample sizes\") + ylab(\"KL divergence estimate\") +      ggtitle(\"Accuracy of different algorithms\") +     theme(plot.title = element_text(hjust = 0.5)) #> Warning: Removed 10 rows containing missing values (`geom_point()`)."},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-1d.html","id":"runtime-of-kl-divergence-estimators","dir":"Articles","previous_headings":"Results","what":"Runtime of KL divergence estimators","title":"Algorithm benchmark in 1D","text":"\\(\\Rightarrow\\) Kernel density-based estimators, use stats::density, generally fastest (except small sample sizes). investigated methods scale approximately linearly sample size, due use fast Fourier transform kernel density estimation use kd-tree nearest neighbours search. bias-reduced nearest neighbour estimator nn_br approximately 1 order magnitude slower 1-nearest neighbour estimator nn_1, without offering additional accuracy 1-D examples. extra effort starts pay higher-dimensional problems.","code":"ggplot(results, aes(x=sample.size, y=runtime, color=algorithm)) +      scale_y_log10() +      geom_jitter(position=position_dodge(.5)) +      facet_wrap(\"distribution\", scales = \"free_y\") +     xlab(\"Sample sizes\") + ylab(\"Runtime [sec]\") +      ggtitle(\"Runtime of different algorithms\") +     theme(plot.title = element_text(hjust = 0.5))"},{"path":[]},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-2d.html","id":"distributions-and-kl-d","dir":"Articles","previous_headings":"Specification of simulation scenario","what":"Distributions and KL-D","title":"Algorithm benchmark in 2D","text":"investigate following pairs distributions, analytical KL divergence values known: \\(\\mathcal{N}\\left(\\begin{pmatrix}0\\\\ 0\\end{pmatrix},\\begin{pmatrix}1 & 0\\\\ 0 & 1\\end{pmatrix}\\right)\\) vs. \\(\\mathcal{N}\\left(\\begin{pmatrix}1\\\\ 1\\end{pmatrix},\\begin{pmatrix}2 & 0\\\\ 0 & 2\\end{pmatrix}\\right)\\) (2D Gaussians, different location/scale, uncorrelated), \\(\\mathcal{N}\\left(\\begin{pmatrix}0\\\\ 0\\end{pmatrix},\\begin{pmatrix}1 & 0.9\\\\ 0.9 & 1\\end{pmatrix}\\right)\\) vs. \\(\\mathcal{N}\\left(\\begin{pmatrix}0\\\\ 0\\end{pmatrix},\\begin{pmatrix}1 & 0.1\\\\ 0.1 & 1\\end{pmatrix}\\right)\\) (2D Gaussians, location/scale, different correlation strength). Analytical values Kullback-Leibler divergences test cases:","code":"p <- list(     indep    = list(mu1 = c(0,0), sigma1 = diag(2),                      mu2 = c(1,1), sigma2 = 2*diag(2)),     corr     = list(mu1 = c(0,0), sigma1 = constDiagMatrix(dim = 2, diag = 1, offDiag = 0.9),                      mu2 = c(0,0), sigma2 = constDiagMatrix(dim = 2, diag = 1, offDiag = 0.1)) ) distributions <- list(     indep = list(         samples = function(n, m) {             X <- MASS::mvrnorm(n = n, mu = p$indep$mu1, Sigma = p$indep$sigma1)             Y <- MASS::mvrnorm(n = m, mu = p$indep$mu2, Sigma = p$indep$sigma2)             list(X = X, Y = Y)         },         kld = do.call(kld_gaussian, p$indep)     ),     corr = list(         samples = function(n, m) {             X <- MASS::mvrnorm(n = n, mu = p$corr$mu1, Sigma = p$corr$sigma1)             Y <- MASS::mvrnorm(n = m, mu = p$corr$mu2, Sigma = p$corr$sigma2)             list(X = X, Y = Y)         },         kld = do.call(kld_gaussian, p$corr)     ) ) kldtrue <- data.frame(distribution = factor(names(distributions),                                              levels = names(distributions),                                             ordered = TRUE),                        kld          = unname(vapply(distributions, function(x) x$kld,1))) kldtrue #>   distribution       kld #> 1        indep 0.6931472 #> 2         corr 0.7445324"},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-2d.html","id":"simulation-scenarios","dir":"Articles","previous_headings":"Specification of simulation scenario","what":"Simulation scenarios","title":"Algorithm benchmark in 2D","text":"distributions specified , samples different sizes drawn, several replicates per distribution sample size.","code":"samplesize <- 10^(2:5) nRep       <- 25L  scenarios <- combinations(     distribution = names(distributions),     sample.size  = samplesize,     replicate    = 1:nRep )"},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-2d.html","id":"algorithms","dir":"Articles","previous_headings":"Specification of simulation scenario","what":"Algorithms","title":"Algorithm benchmark in 2D","text":"following algorithms considered:","code":"algorithms <- list(     kde2  = function(X, Y) kld_est_kde2(X = X, Y = Y),     nn_1  = kld_est_nn,     nn_br = function(X, Y) kld_est_brnn(X = X, Y = Y, warn.max.k = FALSE) ) nAlgo   <- length(algorithms)"},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-2d.html","id":"run-the-simulation-study","dir":"Articles","previous_headings":"","what":"Run the simulation study","title":"Algorithm benchmark in 2D","text":"Post-processing: combine scenarios, kld runtime single data frame Convert factor","code":"# allocating results matrices nscenario  <- nrow(scenarios) runtime <- kld <- matrix(nrow = nscenario,                           ncol = nAlgo,                           dimnames = list(NULL, names(algorithms)))  for (i in 1:nscenario) {      dist <- scenarios$distribution[i]     n    <- scenarios$sample.size[i]          samples <- distributions[[dist]]$sample(n = n, m = n)     X <- samples$X     Y <- samples$Y          # different algorithms are evaluated on the same samples     for (j in 1:nAlgo) {         algo <- algorithms[[j]]         start_time <- Sys.time()         kld[i,j] <- algo(X, Y)         end_time <- Sys.time()         runtime[i,j] <- end_time - start_time     } } tmp1 <- cbind(scenarios, kld) |> melt(measure.vars = names(algorithms),                                       value.name = \"kld\",                                       variable.name = \"algorithm\")  tmp2 <- cbind(scenarios, runtime) |> melt(measure.vars = names(algorithms),                                           value.name = \"runtime\",                                           variable.name = \"algorithm\")  results <- merge(tmp1,tmp2) rm(tmp1,tmp2) results$sample.size <- as.factor(results$sample.size) results$algorithm <- factor(results$algorithm,                              levels = names(algorithms)) results$distribution <- factor(results$distribution,                                 levels = names(distributions),                                 ordered = TRUE)"},{"path":[]},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-2d.html","id":"accuracy-of-kl-divergence-estimators","dir":"Articles","previous_headings":"Results","what":"Accuracy of KL divergence estimators","title":"Algorithm benchmark in 2D","text":"\\(\\Rightarrow\\) estimators converge towards true KL divergence (black solid line). Whereas nearest neighbour-based estimators unbiased, kernel density-based estimator comparable variance considerable bias smaller samples.","code":"ggplot(results, aes(x=sample.size, y=kld, color=algorithm)) +      geom_jitter(position=position_dodge(.5)) +      facet_wrap(\"distribution\", scales = \"free_y\") +     geom_hline(data = kldtrue, aes(yintercept = kld)) +     xlab(\"Sample sizes\") + ylab(\"KL divergence estimate\") +      ggtitle(\"Accuracy of different algorithms\") +     theme(plot.title = element_text(hjust = 0.5))"},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-2d.html","id":"runtime-of-kl-divergence-estimators","dir":"Articles","previous_headings":"Results","what":"Runtime of KL divergence estimators","title":"Algorithm benchmark in 2D","text":"\\(\\Rightarrow\\) kernel density-based estimator, use KernSmooth::bkde, 10-fold faster nearest neighbour-based estimators large samples. However also much less precise.","code":"ggplot(results, aes(x=sample.size, y=runtime, color=algorithm)) +      scale_y_log10() +      geom_jitter(position=position_dodge(.5)) +      facet_wrap(\"distribution\", scales = \"free_y\") +     xlab(\"Sample sizes\") + ylab(\"Runtime [sec]\") +      ggtitle(\"Runtime of different algorithms\") +     theme(plot.title = element_text(hjust = 0.5))"},{"path":[]},{"path":"https://niklhart.github.io/kldest/articles/comparison-1sample-2sample-1D.html","id":"distributions-and-analytical-kl-d","dir":"Articles","previous_headings":"Specification of simulation scenario","what":"Distributions and analytical KL-D","title":"Comparing 1- vs. 2-sample estimation in 1-D","text":"investigate following pairs distributions, analytical KL divergence values known: \\(\\text{Exp}(1)\\) vs. \\(\\text{Exp}(1/12)\\), \\(\\mathcal{N}(0,1)\\) vs. \\(\\mathcal{N}(1,2^2)\\), \\(\\mathcal{U}(1,2)\\) vs. \\(\\mathcal{U}(0,4)\\). Analytical values Kullback-Leibler divergences test cases:","code":"p <- list(     exponential = list(lambda1 = 1, lambda2 = 1/12),     gaussian    = list(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2),     uniform     = list(a1 = 1, b1 = 2, a2 = 0, b2 = 4) ) distributions <- list(     exponential = list(         samples = function(n, m) {             X <- rexp(n, rate = p$exponential$lambda1)             Y <- rexp(m, rate = p$exponential$lambda2)             list(X = X, Y = Y)         },         q = function(x) dexp(x, rate = p$exponential$lambda2),         kld = do.call(kld_exponential, p$exponential)     ),     gaussian = list(         samples = function(n, m) {             X <- rnorm(n, mean = p$gaussian$mu1, sd = sqrt(p$gaussian$sigma1))             Y <- rnorm(m, mean = p$gaussian$mu2, sd = sqrt(p$gaussian$sigma2))             list(X = X, Y = Y)         },         q = function(x) dnorm(x, mean = p$gaussian$mu2, sd = sqrt(p$gaussian$sigma2)),         kld = do.call(kld_gaussian, p$gaussian)     ),     uniform = list(          samples = function(n, m) {             X <- runif(n, min = p$uniform$a1, max = p$uniform$b1)             Y <- runif(m, min = p$uniform$a2, max = p$uniform$b2)             list(X = X, Y = Y)         },         q = function(x) dunif(x, min = p$uniform$a2, max = p$uniform$b2),         kld = do.call(kld_uniform, p$uniform)     ) ) vapply(distributions, function(x) x$kld, 1) #> exponential    gaussian     uniform  #>   1.5682400   0.4431472   1.3862944"},{"path":"https://niklhart.github.io/kldest/articles/comparison-1sample-2sample-1D.html","id":"simulation-scenarios","dir":"Articles","previous_headings":"Specification of simulation scenario","what":"Simulation scenarios","title":"Comparing 1- vs. 2-sample estimation in 1-D","text":"distributions specified , samples different sizes drawn, several replicates per distribution sample size.","code":"samplesize <- 10^(2:4) nRep       <- 25L  scenarios <- combinations(     distribution = names(distributions),     sample.size  = samplesize,     replicate    = 1:nRep )"},{"path":"https://niklhart.github.io/kldest/articles/comparison-1sample-2sample-1D.html","id":"algorithms","dir":"Articles","previous_headings":"Specification of simulation scenario","what":"Algorithms","title":"Comparing 1- vs. 2-sample estimation in 1-D","text":"following algorithms considered:","code":"algorithms_XY <- list(     nn_XY     = function(X, Y) kld_est_nn(X, Y) ) algorithms_Xq <- list(     nn_Xq = function(X, q) kld_est_nn(X, q = q) ) nAlgoXY  <- length(algorithms_XY) nAlgoXq  <- length(algorithms_Xq) nmAlgo   <- c(names(algorithms_XY),names(algorithms_Xq))"},{"path":[]},{"path":"https://niklhart.github.io/kldest/articles/comparison-1sample-2sample-1D.html","id":"simulation","dir":"Articles","previous_headings":"Comparison","what":"Simulation","title":"Comparing 1- vs. 2-sample estimation in 1-D","text":"Post-processing: combine scenarios, kld runtime single data frame","code":"# allocating results matrices nscenario  <- nrow(scenarios) runtime <- kld <- matrix(nrow = nscenario,                           ncol = nAlgoXY+nAlgoXq,                           dimnames = list(NULL, nmAlgo))  for (i in 1:nscenario) {      dist <- scenarios$distribution[i]     n    <- scenarios$sample.size[i]          samples <- distributions[[dist]]$sample(n = n, m = n)     X <- samples$X     Y <- samples$Y     q <- distributions[[dist]]$q      # different algorithms are evaluated on the same samples     for (j in 1:nAlgoXY) {         algo         <- algorithms_XY[[j]]         start_time   <- Sys.time()         kld[i,j]     <- algo(X, Y)         end_time     <- Sys.time()         runtime[i,j] <- end_time - start_time     }     for (j in 1:nAlgoXq) {         nj            <- nAlgoXY+j         algo          <- algorithms_Xq[[j]]         start_time    <- Sys.time()         kld[i,nj]     <- algo(X, q)         end_time      <- Sys.time()         runtime[i,nj] <- end_time - start_time     } } tmp1 <- cbind(scenarios, kld) |> melt(measure.vars = nmAlgo,                                       value.name = \"kld\",                                       variable.name = \"algorithm\")  tmp2 <- cbind(scenarios, runtime) |> melt(measure.vars = nmAlgo,                                           value.name = \"runtime\",                                           variable.name = \"algorithm\")  results <- merge(tmp1,tmp2) results$sample.size <- as.factor(results$sample.size) rm(tmp1,tmp2)"},{"path":"https://niklhart.github.io/kldest/articles/comparison-1sample-2sample-1D.html","id":"results-accuracy-of-kl-divergence-estimators","dir":"Articles","previous_headings":"Comparison","what":"Results: Accuracy of KL divergence estimators","title":"Comparing 1- vs. 2-sample estimation in 1-D","text":"\\(\\Rightarrow\\) 1-sample estimation accurate 2-sample estimation three 1-D test cases sample sizes, expected since information available 1-sample case. However, notice bias reduction kld_est_brnn possible 1-sample estimation. Hence, high-dimensional cases density approximate distribution \\(Q\\) available, may preferable simulate (large) sample \\(Q\\) use bias reduction two-sample problem, rather using kld_est_nn 1-sample variant.","code":"ggplot(results, aes(x=sample.size, y=kld, color=algorithm)) +      geom_jitter(position=position_dodge(.5)) +      facet_wrap(\"distribution\", scales = \"free_y\") +     geom_hline(data = data.frame(distribution = names(distributions),                                   kldtrue = vapply(distributions, function(x) x$kld,1)),                 aes(yintercept = kldtrue)) +     xlab(\"Sample sizes\") + ylab(\"KL divergence estimate\") +      ggtitle(\"Accuracy of different algorithms\") +     theme(plot.title = element_text(hjust = 0.5))"},{"path":"https://niklhart.github.io/kldest/articles/uncertainty-quantification.html","id":"simulation-scenarios","dir":"Articles","previous_headings":"","what":"Simulation scenarios","title":"Confidence intervals for KL divergence","text":"distributions specified , samples different sizes drawn, several replicates per distribution sample size.","code":"samplesize <- c(10,100,1000,10000) nRep       <- 100L  scenarios <- combinations(     distribution = names(distributions),     sample.size  = samplesize,     replicate    = 1:nRep )"},{"path":"https://niklhart.github.io/kldest/articles/uncertainty-quantification.html","id":"algorithm-bootstrap-methods","dir":"Articles","previous_headings":"","what":"Algorithm & bootstrap methods","title":"Confidence intervals for KL divergence","text":"following estimator considered: use following subsampling variants:","code":"estimator <- kld_est_nn resampling <- list(     sub_n12 = function(...) kld_ci_subsampling(..., subsample.size = function(n) n^(1/2)),      sub_n23 = function(...) kld_ci_subsampling(..., subsample.size = function(n) n^(2/3)),      sub_n12se = function(...) kld_ci_subsampling(..., subsample.size = function(n) n^(1/2), method = \"se\"),     sub_n23se = function(...) kld_ci_subsampling(..., subsample.size = function(n) n^(2/3), method = \"se\") ) nResamp <- length(resampling)"},{"path":[]},{"path":"https://niklhart.github.io/kldest/articles/uncertainty-quantification.html","id":"calculation-of-empirical-coverage","dir":"Articles","previous_headings":"Uncertainty quantification of estimators","what":"Calculation of empirical coverage","title":"Confidence intervals for KL divergence","text":"Combine scenarios CI coverage information: Compute coverage per sample size / algorithm / distribution:","code":"# allocating results matrices nscenario  <- nrow(scenarios) covered <- matrix(nrow = nscenario,                    ncol = nResamp,                    dimnames = list(NULL, names(resampling)))  # looping over scenarios for (i in 1:nscenario) {      dist <- scenarios$distribution[i]     n    <- scenarios$sample.size[i]          samples <- distributions[[dist]]$sample(n = n, m = n)     kld     <- distributions[[dist]]$kld          X <- samples$X     Y <- samples$Y          # different algorithms are evaluated on the same samples     for (j in 1:nResamp) {         kldboot <- resampling[[j]](X, Y, estimator = estimator, B = 500L)         covered[i,j] <- kldboot$ci[1] <= kld && kld <= kldboot$ci[2]     } } results <- cbind(scenarios, covered) |> melt(measure.vars  = names(resampling),                                              value.name    = \"covered\",                                              variable.name = \"resampling\") coverage <- dcast(results,                    sample.size + resampling + distribution ~ .,                    value.var = \"covered\",                    fun.aggregate = function(x) mean(x, na.rm = TRUE)) names(coverage)[4] <- \"coverage\""},{"path":"https://niklhart.github.io/kldest/articles/uncertainty-quantification.html","id":"results-coverage-of-confidence-intervals-for-kl-divergence","dir":"Articles","previous_headings":"Uncertainty quantification of estimators","what":"Results: coverage of confidence intervals for KL divergence","title":"Confidence intervals for KL divergence","text":"\\(\\Rightarrow\\) coverage confidence intervals based nearest neighbour density estimation approaches nominal coverage 95% increasing sample sizes. Gaussian case, kernel density based method performs well estimation benchmark, also holds kernel density method. However, due bias estimating KL divergence exponential uniform distributions, uncertainty quantification also fails. using bootstrapping instead subsampling, results similar kernel density estimation, change dramatically nearest neighbour density estimation, deal ties produced resampling replacement.","code":"ggplot(coverage, aes(x = sample.size, y = coverage, color = resampling)) +      facet_wrap(\"distribution\") +     geom_line() +      scale_x_log10() +     geom_hline(yintercept = 0.95, lty = 2) +      scale_color_discrete(name = \"CI method\",                          labels = c(\"Reverse percentile, s = n^(1/2)\",                                    \"Reverse percentile, s = n^(2/3)\",                                    \"1.96 standard error, s = n^(1/2)\",                                    \"1.96 standard error, s = n^(2/3)\")) +     ggtitle(\"Subsampling-based confidence intervals\") +     theme(plot.title = element_text(hjust = 0.5))"},{"path":"https://niklhart.github.io/kldest/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Niklas Hartung. Author, maintainer, copyright holder.","code":""},{"path":"https://niklhart.github.io/kldest/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Hartung N (2023). kldest: Sample-based estimation Kullback-Leibler divergence. R package version 0.1.0.9002, https://niklhart.github.io/kldest/.","code":"@Manual{,   title = {kldest: Sample-based estimation of Kullback-Leibler divergence},   author = {Niklas Hartung},   year = {2023},   note = {R package version 0.1.0.9002},   url = {https://niklhart.github.io/kldest/}, }"},{"path":"https://niklhart.github.io/kldest/index.html","id":"kldest-kullback-leibler-divergence-estimation","dir":"","previous_headings":"","what":"Sample-based estimation of Kullback-Leibler divergence","title":"Sample-based estimation of Kullback-Leibler divergence","text":"goal kldest estimate Kullback-Leibler (KL) divergence DKL(P||Q) two probability distributions P Q based : sample x1, ..., xn P probability density q Q, samples x1, ..., xn P y1, ..., ym Q. distributions P Q may uni- multivariate, may discrete, continuous mixed discrete/continuous. Different estimation algorithms provided continuous distributions, either based nearest neighbour density estimation kernel density estimation. Confidence intervals KL divergence can also computed, either via subsampling (preferred) bootstrapping.","code":""},{"path":"https://niklhart.github.io/kldest/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Sample-based estimation of Kullback-Leibler divergence","text":"can install development version kldest GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"niklhart/kldest\")"},{"path":"https://niklhart.github.io/kldest/index.html","id":"a-minimal-example-for-kl-divergence-estimation","dir":"","previous_headings":"","what":"A minimal example for KL divergence estimation","title":"Sample-based estimation of Kullback-Leibler divergence","text":"KL divergence estimation based nearest neighbour density estimates flexible approach.","code":"library(kldest)"},{"path":"https://niklhart.github.io/kldest/index.html","id":"kl-divergence-between-1-d-gaussians","dir":"","previous_headings":"A minimal example for KL divergence estimation","what":"KL divergence between 1-D Gaussians","title":"Sample-based estimation of Kullback-Leibler divergence","text":"Analytical KL divergence: Estimate based two samples Gaussians: Estimate based sample first Gaussian density second: Uncertainty quantification via subsampling:","code":"kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2) #> [1] 0.4431472 X <- rnorm(100) Y <- rnorm(100, mean = 1, sd = 2) kld_est_nn(X, Y) #> [1] 0.630473 q <- function(x) dnorm(x, mean = 1, sd =2) kld_est_nn(X, q = q) #> [1] 0.3904603 kld_ci_subsampling(X, q = q)$ci #>       2.5%      97.5%  #> 0.06544115 0.65278508"},{"path":"https://niklhart.github.io/kldest/index.html","id":"kl-divergence-between-2-d-gaussians","dir":"","previous_headings":"A minimal example for KL divergence estimation","what":"KL divergence between 2-D Gaussians","title":"Sample-based estimation of Kullback-Leibler divergence","text":"Analytical KL divergence uncorrelated correlated Gaussian: Estimate based two samples Gaussians:","code":"kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),              mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2)) #> [1] 0.5 X1 <- rnorm(100) X2 <- rnorm(100) Y1 <- rnorm(100) Y2 <- Y1 + rnorm(100) X <- cbind(X1,X2) Y <- cbind(Y1,Y2)  kld_est_nn(X, Y) #> [1] 0.2497069"},{"path":"https://niklhart.github.io/kldest/reference/combinations.html","id":null,"dir":"Reference","previous_headings":"","what":"Combinations of input arguments — combinations","title":"Combinations of input arguments — combinations","text":"Combinations input arguments","code":""},{"path":"https://niklhart.github.io/kldest/reference/combinations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combinations of input arguments — combinations","text":"","code":"combinations(...)"},{"path":"https://niklhart.github.io/kldest/reference/combinations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combinations of input arguments — combinations","text":"... number atomic vectors.","code":""},{"path":"https://niklhart.github.io/kldest/reference/combinations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combinations of input arguments — combinations","text":"data frame columns named inputs, containing input combinations.","code":""},{"path":"https://niklhart.github.io/kldest/reference/combinations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Combinations of input arguments — combinations","text":"","code":"combinations(a = 1:2, b = letters[1:3], c = LETTERS[1:2]) #>    a b c #> 1  1 a A #> 2  2 a A #> 3  1 b A #> 4  2 b A #> 5  1 c A #> 6  2 c A #> 7  1 a B #> 8  2 a B #> 9  1 b B #> 10 2 b B #> 11 1 c B #> 12 2 c B"},{"path":"https://niklhart.github.io/kldest/reference/constDiagMatrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Constant plus diagonal matrix — constDiagMatrix","title":"Constant plus diagonal matrix — constDiagMatrix","text":"Specify matrix constant values diagonal -diagonals. matrices can used vary degree dependency covariate matrices, example evaluating accuracy KL-divergence estimation algorithms.","code":""},{"path":"https://niklhart.github.io/kldest/reference/constDiagMatrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Constant plus diagonal matrix — constDiagMatrix","text":"","code":"constDiagMatrix(dim = 1, diag = 1, offDiag = 0)"},{"path":"https://niklhart.github.io/kldest/reference/constDiagMatrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Constant plus diagonal matrix — constDiagMatrix","text":"dim Dimension diag Value diagonal offDiag Value -diagonals","code":""},{"path":"https://niklhart.github.io/kldest/reference/constDiagMatrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Constant plus diagonal matrix — constDiagMatrix","text":"dim--dim matrix","code":""},{"path":"https://niklhart.github.io/kldest/reference/constDiagMatrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Constant plus diagonal matrix — constDiagMatrix","text":"","code":"constDiagMatrix(dim = 3, diag = 1, offDiag = 0.9) #>      [,1] [,2] [,3] #> [1,]  1.0  0.9  0.9 #> [2,]  0.9  1.0  0.9 #> [3,]  0.9  0.9  1.0"},{"path":"https://niklhart.github.io/kldest/reference/is_two_sample.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect if a one- or two-sample problem is specified — is_two_sample","title":"Detect if a one- or two-sample problem is specified — is_two_sample","text":"Detect one- two-sample problem specified","code":""},{"path":"https://niklhart.github.io/kldest/reference/is_two_sample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect if a one- or two-sample problem is specified — is_two_sample","text":"","code":"is_two_sample(Y, q)"},{"path":"https://niklhart.github.io/kldest/reference/is_two_sample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect if a one- or two-sample problem is specified — is_two_sample","text":"Y vector, matrix, data frame NULL q function NULL.","code":""},{"path":"https://niklhart.github.io/kldest/reference/is_two_sample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect if a one- or two-sample problem is specified — is_two_sample","text":"TRUE two-sample problem (.e., Y non-null q = NULL) FALSE one-sample problem (.e., Y = NULL q non-null).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_bootstrap.html","id":null,"dir":"Reference","previous_headings":"","what":"Uncertainty of KL divergence estimate using Efron's bootstrap. — kld_ci_bootstrap","title":"Uncertainty of KL divergence estimate using Efron's bootstrap. — kld_ci_bootstrap","text":"function computes confidence interval KL divergence based Efron's bootstrap. approach works kernel density-based estimators since nearest neighbour-based estimators deal ties produced sampling replacement.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_bootstrap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Uncertainty of KL divergence estimate using Efron's bootstrap. — kld_ci_bootstrap","text":"","code":"kld_ci_bootstrap(X, Y, estimator = kld_est_kde1, B = 500L, alpha = 0.05)"},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_bootstrap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Uncertainty of KL divergence estimate using Efron's bootstrap. — kld_ci_bootstrap","text":"X, Y n--d m--d matrices, representing n samples true distribution \\(P\\) m samples approximate distribution \\(Q\\), d dimensions. Vector input treated column matrix. estimator function expecting two inputs X Y, Kullback-Leibler divergence estimation method. Defaults kld_est_kde1, can deal one-dimensional two-sample problems (.e., d = 1 q = NULL). B Number bootstrap replicates (default: 500), larger, accurate, also computationally expensive. alpha Error level, defaults 0.05.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_bootstrap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Uncertainty of KL divergence estimate using Efron's bootstrap. — kld_ci_bootstrap","text":"list fields \"est\" (estimated KL divergence), \"boot\" (length B numeric vector KL divergence estimates bootstrap samples), \"ci\" (length 2 vector containing lower upper limits estimated confidence interval).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_bootstrap.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Uncertainty of KL divergence estimate using Efron's bootstrap. — kld_ci_bootstrap","text":"Reference: Efron, \"Bootstrap Methods: Another Look Jackknife\", Annals Statistics, Vol. 7, . 1 (1979).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_bootstrap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Uncertainty of KL divergence estimate using Efron's bootstrap. — kld_ci_bootstrap","text":"","code":"# 1D Gaussian, two samples X <- rnorm(100) Y <- rnorm(100, mean = 1, sd = 2) kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2) #> [1] 0.4431472 kld_est_kde1(X, Y) #> [1] 0.3934408 kld_ci_bootstrap(X, Y) #> $est #> [1] 0.3934408 #>  #> $boot #>   [1] 0.4886329 0.4483070 0.4232641 0.3620615 0.5024286 0.3247789 0.5890724 #>   [8] 0.5127156 0.4666501 0.3986273 0.3899138 0.5142621 0.3512228 0.3621353 #>  [15] 0.4609882 0.2431704 0.3333234 0.3809732 0.4037258 0.4812337 0.4149827 #>  [22] 0.5624474 0.3792913 0.5072933 0.3377614 0.5394949 0.4874579 0.4984601 #>  [29] 0.4073645 0.3964788 0.4546963 0.2907430 0.4587043 0.3248458 0.4248938 #>  [36] 0.4088349 0.4136520 0.5285499 0.3577385 0.5321580 0.4729660 0.3073300 #>  [43] 0.3248748 0.3185047 0.3932537 0.3049262 0.3431770 0.4234037 0.3596289 #>  [50] 0.5300989 0.4345493 0.4705799 0.4512874 0.6125461 0.2758610 0.3932345 #>  [57] 0.4464103 0.5343487 0.5033341 0.6389338 0.4499595 0.4588224 0.5868964 #>  [64] 0.4014555 0.5925237 0.3686151 0.4478998 0.4194886 0.5624580 0.4508342 #>  [71] 0.4277825 0.5367627 0.4027787 0.8109231 0.3942527 0.3574392 0.5935286 #>  [78] 0.4875295 0.4325926 0.3539418 0.4305397 0.3459871 0.3469749 0.3921713 #>  [85] 0.3431420 0.5991984 0.3998306 0.6377260 0.3957341 0.5380287 0.4153252 #>  [92] 0.3777003 0.4496862 0.4425852 0.6021612 0.3847954 0.3119121 0.4491915 #>  [99] 0.5339704 0.4213324 0.4273720 0.5589864 0.3782274 0.3223902 0.4789983 #> [106] 0.4847712 0.3863508 0.3233168 0.4296727 0.3697199 0.3791807 0.3751872 #> [113] 0.4515264 0.4714660 0.4144020 0.4871517 0.4127353 0.4905011 0.4572996 #> [120] 0.5431174 0.4388609 0.3886652 0.4840604 0.4051513 0.4889390 0.3508135 #> [127] 0.4526828 0.3604485 0.3586931 0.3468218 0.3037172 0.4331172 0.5825941 #> [134] 0.5724989 0.3722806 0.6710964 0.3443965 0.3759596 0.4556057 0.2991018 #> [141] 0.4075453 0.3780365 0.4768731 0.6456378 0.5722233 0.4723172 0.4574301 #> [148] 0.3445001 0.4302370 0.3933242 0.5288292 0.5480542 0.5392452 0.2731205 #> [155] 0.4696666 0.3699395 0.3886615 0.5092006 0.5117402 0.3655987 0.4581989 #> [162] 0.3868585 0.4006406 0.4613085 0.4691810 0.3996410 0.4067158 0.3756025 #> [169] 0.3820023 0.5115211 0.4797162 0.4296086 0.4228243 0.4267347 0.4449900 #> [176] 0.3516484 0.4934200 0.3290382 0.5169770 0.3703643 0.3210406 0.4059977 #> [183] 0.3247546 0.3498705 0.4008317 0.4201269 0.4191837 0.7159320 0.4585076 #> [190] 0.4790003 0.4939683 0.4510036 0.3720198 0.5129331 0.4497861 0.5082421 #> [197] 0.4665342 0.4423768 0.4468042 0.3858770 0.5129382 0.4139191 0.4905997 #> [204] 0.3739085 0.3850606 0.4717237 0.6702995 0.3287166 0.4131615 0.4847844 #> [211] 0.3917543 0.3614667 0.7112052 0.4289175 0.4739092 0.3518731 0.4033720 #> [218] 0.5844451 0.3430080 0.4797176 0.4209393 0.4998412 0.4484860 0.4545414 #> [225] 0.3843360 0.4565002 0.4719151 0.4833490 0.6912769 0.4424756 0.5141172 #> [232] 0.4103308 0.2399364 0.5429829 0.5048642 0.4485994 0.4763519 0.5700370 #> [239] 0.6926265 0.5496328 0.3523791 0.2640336 0.5297683 0.4143330 0.5075449 #> [246] 0.5329096 0.4611086 0.2763586 0.5073686 0.3962765 0.3335818 0.4615537 #> [253] 0.4589759 0.5397958 0.5018397 0.4279975 0.4295101 0.3159674 0.2980475 #> [260] 0.4856307 0.5707091 0.5232817 0.2414152 0.3253277 0.3806309 0.3627191 #> [267] 0.4855952 0.4882354 0.3001428 0.5818008 0.4144581 0.5342959 0.3930882 #> [274] 0.4289656 0.3980270 0.6231399 0.4224412 0.6268262 0.4918140 0.4770939 #> [281] 0.4536892 0.4202504 0.3816306 0.4041312 0.4149460 0.4145319 0.4252975 #> [288] 0.4664944 0.3611774 0.4561025 0.6304078 0.2652205 0.5685151 0.4111676 #> [295] 0.3138042 0.5160670 0.4605141 0.6217554 0.5311958 0.3986934 0.3954607 #> [302] 0.5399475 0.3103695 0.4442134 0.3228179 0.3293924 0.4143836 0.4767824 #> [309] 0.6541213 0.4210080 0.3208788 0.4185165 0.3754221 0.4647923 0.5264907 #> [316] 0.4993216 0.4035309 0.3101842 0.3967718 0.3194636 0.4068056 0.4704214 #> [323] 0.3711335 0.5869320 0.3305733 0.4119520 0.3617838 0.3474294 0.4545695 #> [330] 0.3831070 0.3337931 0.4242702 0.3765105 0.2693770 0.3424250 0.4171692 #> [337] 0.3469522 0.6332515 0.4285286 0.2876559 0.6198475 0.4308479 0.3773330 #> [344] 0.5179640 0.6297760 0.3436261 0.6015426 0.3646294 0.3341479 0.5814531 #> [351] 0.6407427 0.6025866 0.3284910 0.4373004 0.4804008 0.5205557 0.5062690 #> [358] 0.6052471 0.4265555 0.4741538 0.4708551 0.2183528 0.3319383 0.4775773 #> [365] 0.5314762 0.4031699 0.4217502 0.2970438 0.3923107 0.5519074 0.6674154 #> [372] 0.3817207 0.4274865 0.3809294 0.5825801 0.3661895 0.5645880 0.3683355 #> [379] 0.3215623 0.4360517 0.4594620 0.4669543 0.3385619 0.4785928 0.4825927 #> [386] 0.4208846 0.6402825 0.3139751 0.4135892 0.3988122 0.4211548 0.4094673 #> [393] 0.4297053 0.3395300 0.5050353 0.3885304 0.3118519 0.3899191 0.4647480 #> [400] 0.4137751 0.3972584 0.3368188 0.6251173 0.3070543 0.3306304 0.4520097 #> [407] 0.4308207 0.3811300 0.4510347 0.3265746 0.2963095 0.2058010 0.2816813 #> [414] 0.3069044 0.3004909 0.4120108 0.3287452 0.5453376 0.5439627 0.4757392 #> [421] 0.4805770 0.3199303 0.2936604 0.4217032 0.3396629 0.3085770 0.6239182 #> [428] 0.3856734 0.3037751 0.4370785 0.3014074 0.6272860 0.5245406 0.3988096 #> [435] 0.2435601 0.4907351 0.6328099 0.4014397 0.3499354 0.3187712 0.4412657 #> [442] 0.3631474 0.4116528 0.2739409 0.5060164 0.4281730 0.4166488 0.3732067 #> [449] 0.5106492 0.2470415 0.4154264 0.4241199 0.4464277 0.4385605 0.5102210 #> [456] 0.2879305 0.3564871 0.4205240 0.4437646 0.4496512 0.4101541 0.4134545 #> [463] 0.5667555 0.3384108 0.3251606 0.4067043 0.2294637 0.4444374 0.4765790 #> [470] 0.2911961 0.4286402 0.4184559 0.4005862 0.5145582 0.4188014 0.5647444 #> [477] 0.4497756 0.3776700 0.4518866 0.3263581 0.3983592 0.3159667 0.3785415 #> [484] 0.2865616 0.4038763 0.3820890 0.3389952 0.4083094 0.4719184 0.3121665 #> [491] 0.5535178 0.3029963 0.5711966 0.2536862 0.2494167 0.4007204 0.4367163 #> [498] 0.4701257 0.3603011 0.3693400 #>  #> $ci #>     97.5%      2.5%  #> 0.1485215 0.5157265  #>  #> $se #> [1] 0.09441741 #>"},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_subsampling.html","id":null,"dir":"Reference","previous_headings":"","what":"Uncertainty of KL divergence estimate using Politis/Romano's subsampling bootstrap. — kld_ci_subsampling","title":"Uncertainty of KL divergence estimate using Politis/Romano's subsampling bootstrap. — kld_ci_subsampling","text":"function computes confidence interval KL divergence based subsampling bootstrap introduced Politis Romano. See Details theoretical properties method.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_subsampling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Uncertainty of KL divergence estimate using Politis/Romano's subsampling bootstrap. — kld_ci_subsampling","text":"","code":"kld_ci_subsampling(   X,   Y = NULL,   q = NULL,   estimator = kld_est_nn,   B = 500L,   alpha = 0.05,   subsample.size = function(x) x^(2/3),   convergence.rate = sqrt,   method = c(\"quantile\", \"se\"),   n.cores = 1L )"},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_subsampling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Uncertainty of KL divergence estimate using Politis/Romano's subsampling bootstrap. — kld_ci_subsampling","text":"X, Y n--d m--d matrices, representing n samples true distribution \\(P\\) m samples approximate distribution \\(Q\\), d dimensions. Vector input treated column matrix. Y can left blank q specified (see ). q density function approximate distribution \\(Q\\). Either Y q must specified. estimator Kullback-Leibler divergence estimation method; function expecting two inputs (X Y q, depending arguments provided). Defaults kld_est_nn. B Number bootstrap replicates (default: 500), larger, accurate, also computationally expensive. alpha Error level, defaults 0.05. subsample.size function specifying size subsamples, defaults \\(f(x) = x^{2/3}\\). convergence.rate function computing convergence rate estimator function sample sizes. Defaults \\(f(x) = x^{1/2}\\). method Either \"quantile\" (default), also known reverse percentile method, \"se\" normal approximation KL divergence estimator using standard error subsamples. n.cores Number cores use parallel computing (defaults 1, means parallel computing used). use option, parallel package must installed OS must UNIX type (.e., Windows). Otherwise, n.cores reset 1, message.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_subsampling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Uncertainty of KL divergence estimate using Politis/Romano's subsampling bootstrap. — kld_ci_subsampling","text":"list following fields: \"est\" (estimated KL divergence), \"boot\" (length B numeric vector KL divergence estimates bootstrap subsamples), \"ci\" (length 2 vector containing lower upper limits estimated confidence interval).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_subsampling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Uncertainty of KL divergence estimate using Politis/Romano's subsampling bootstrap. — kld_ci_subsampling","text":"general terms, tetting \\(b_n\\) subsample size sample size \\(n\\), \\(\\tau_n\\) convergence rate estimator, confidence interval calculated subsampling asymptotic coverage \\(1 - \\alpha\\) long \\(b_n/n\\rightarrow 0\\), \\(b_n\\rightarrow\\infty\\) \\(\\frac{\\tau_{b_n}}{\\tau_n}\\rightarrow 0\\). convergence rate nearest-neighbour based KL divergence estimator \\(\\tau_n = \\sqrt{n}\\), condition subsample size reduces \\(b_n/n\\rightarrow 0\\) \\(b_n\\rightarrow\\infty\\). default, \\(b_n = n^{2/3}\\). two-sample problem, \\(n\\) \\(b_n\\) replaced effective sample sizes \\(n_\\text{eff} = \\min(n,m)\\) \\(b_{n,\\text{eff}} = \\min(b_n,b_m)\\). Reference: Politis Romano, \"Large sample confidence regions based subsamples minimal assumptions\", Annals Statistics, Vol. 22, . 4 (1994).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_subsampling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Uncertainty of KL divergence estimate using Politis/Romano's subsampling bootstrap. — kld_ci_subsampling","text":"","code":"# 1D Gaussian (one- and two-sample problems) X <- rnorm(100) Y <- rnorm(100, mean = 1, sd = 2) q <- function(x) dnorm(x, mean =1, sd = 2) kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2) #> [1] 0.4431472 kld_est_nn(X, Y = Y) #> [1] 0.4350666 kld_est_nn(X, q = q) #> [1] 0.4986495 kld_ci_subsampling(X, Y)$ci #>        2.5%       97.5%  #> -0.04365055  0.85876845  kld_ci_subsampling(X, q = q)$ci #>      2.5%     97.5%  #> 0.1889664 0.7712116"},{"path":"https://niklhart.github.io/kldest/reference/kld_discrete.html","id":null,"dir":"Reference","previous_headings":"","what":"Analytical KL divergence for two discrete distributions — kld_discrete","title":"Analytical KL divergence for two discrete distributions — kld_discrete","text":"Analytical KL divergence two discrete distributions","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_discrete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analytical KL divergence for two discrete distributions — kld_discrete","text":"","code":"kld_discrete(P, Q)"},{"path":"https://niklhart.github.io/kldest/reference/kld_discrete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analytical KL divergence for two discrete distributions — kld_discrete","text":"P, Q Numerical arrays dimensions, representing discrete probability distributions","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_discrete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analytical KL divergence for two discrete distributions — kld_discrete","text":"scalar (Kullback-Leibler divergence)","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_discrete.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analytical KL divergence for two discrete distributions — kld_discrete","text":"","code":"# 1-D example P <- 1:4/10 Q <- rep(0.25,4) kld_discrete(P,Q) #> [1] 0.1064401  # The above example in 2-D P <- matrix(1:4/10,nrow=2) Q <- matrix(0.25,nrow=2,ncol=2) kld_discrete(P,Q) #> [1] 0.1064401"},{"path":"https://niklhart.github.io/kldest/reference/kld_est.html","id":null,"dir":"Reference","previous_headings":"","what":"Kullback-Leibler divergence estimator for discrete, continuous or mixed data. — kld_est","title":"Kullback-Leibler divergence estimator for discrete, continuous or mixed data. — kld_est","text":"two mixed continuous/discrete distributions densities \\(p\\) \\(q\\), denoting \\(x = (x_\\text{c},x_\\text{d})\\), Kullback-Leibler divergence \\(D_{KL}(p||q)\\) given $$D_{KL}(p||q) = \\sum_{x_d} \\int p(x_c,x_d) \\log\\left(\\frac{p(x_c,x_d)}{q(x_c,x_d)}\\right)dx_c.$$ Conditioning discrete variables \\(x_d\\), can re-written $$D_{KL}(p||q) = \\sum_{x_d} p(x_d) D_{KL}\\big(p(\\cdot|x_d)||q(\\cdot|x_d)\\big) + D_{KL}\\big(p_{x_d}||q_{x_d}\\big).$$ , terms $$D_{KL}\\big(p(\\cdot|x_d)||q(\\cdot|x_d)\\big)$$ approximated via nearest neighbour- kernel-based density estimates datasets X Y stratified discrete variables, $$D_{KL}\\big(p_{x_d}||q_{x_d}\\big)$$ approximated using relative frequencies.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kullback-Leibler divergence estimator for discrete, continuous or mixed data. — kld_est","text":"","code":"kld_est(   X,   Y = NULL,   q = NULL,   estimator.continuous = kld_est_nn,   estimator.discrete = kld_est_discrete,   vartype = NULL )"},{"path":"https://niklhart.github.io/kldest/reference/kld_est.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kullback-Leibler divergence estimator for discrete, continuous or mixed data. — kld_est","text":"X, Y Data frames matrices number columns d (multivariate samples), numeric/character vectors (univariate samples, .e. d=1), representing n samples true distribution \\(P\\) m samples approximate distribution \\(Q\\), d dimensions. Y can left blank q specified (see ). q density function approximate distribution \\(Q\\). Either Y q must specified. general, q must given decomposed form, \\(q(y_c|y_d)q(y_d)\\), specified named list field cond conditional density \\(q(y_c|y_d)\\) (function expects two arguments y_c y_d) disc discrete marginal density \\(q(y_d)\\) (function expects one argument y_d). possible, may preferable simulate large sample \\(Q\\) use two-sample syntax instead. compatibility continuous discrete one-sample estimators, sample(s) /continuous discrete, instead specifying q length 2 list, may also given function handle computing continous discrete density. estimator.continuous, estimator.discrete KL divergence estimators continuous discrete data, respectively. function two arguments X Y X q, depending whether two-sample one-sample problem considered. Defaults kld_est_nn kld_est_discrete, respectively. vartype length d character vector, vartype[] = \"c\" meaning -th variable continuous, vartype[] = \"d\" meaning discrete. unspecified, vartype \"c\" numeric columns \"d\" character factor columns. default work levels discrete variables encoded using numbers (e.g., 0 females 1 males) count data.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Kullback-Leibler divergence estimator for discrete, continuous or mixed data. — kld_est","text":"scalar, estimated Kullback-Leibler divergence \\(\\hat D_{KL}(P||Q)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Kullback-Leibler divergence estimator for discrete, continuous or mixed data. — kld_est","text":"","code":"# 2D example, two samples X <- data.frame(cont  = rnorm(10),                 discr = c(rep('a',4),rep('b',6))) Y <- data.frame(cont  = c(rnorm(5), rnorm(5, sd = 2)),                 discr = c(rep('a',5),rep('b',5))) kld_est(X, Y) #> [1] 0.4482071  # 2D example, one sample X <- data.frame(cont  = rnorm(10),                 discr = c(rep(0,4),rep(1,6))) q <- list(cond = function(xc,xd) dnorm(xc, mean = xd, sd = 1),           disc = function(xd) dbinom(xd, size = 1, prob = 0.5)) kld_est(X, q = q, vartype = c(\"c\",\"d\")) #> [1] 0.4875457"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_brnn.html","id":null,"dir":"Reference","previous_headings":"","what":"Bias-reduced generalized k-nearest-neighbour KL divergence estimation — kld_est_brnn","title":"Bias-reduced generalized k-nearest-neighbour KL divergence estimation — kld_est_brnn","text":"bias-reduced generalized k-NN based KL divergence estimator Wang et al. (2009) specified Eq.(29).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_brnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bias-reduced generalized k-nearest-neighbour KL divergence estimation — kld_est_brnn","text":"","code":"kld_est_brnn(X, Y, max.k = 100, warn.max.k = TRUE, eps = 0)"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_brnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bias-reduced generalized k-nearest-neighbour KL divergence estimation — kld_est_brnn","text":"X, Y n--d m--d matrices, representing n samples true distribution \\(P\\) m samples approximate distribution \\(Q\\), d dimensions. Vector input treated column matrix. Y can left blank q specified (see ). max.k Maximum numbers nearest neighbours compute (default: 100). larger max.k may yield accurate KL-D estimate (see warn.max.k), always increase computational cost. warn.max.k TRUE (default), warns max.k max.k neighbours within neighbourhood \\(\\delta\\) data point(s). case, first max.k neighbours counted. consequence, max.k may required increased. eps Error bound nearest neighbour search. value eps = 0 (default) implies exact nearest neighbour search, eps > 0 approximate nearest neighbours sought, may somewhat faster high-dimensional problems.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_brnn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bias-reduced generalized k-nearest-neighbour KL divergence estimation — kld_est_brnn","text":"scalar, estimated Kullback-Leibler divergence \\(\\hat D_{KL}(P||Q)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_brnn.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bias-reduced generalized k-nearest-neighbour KL divergence estimation — kld_est_brnn","text":"Finite sample bias reduction achieved adaptive choice number nearest neighbours. Fixing number nearest neighbours upfront, done kld_est_nn(), may result different distances \\(\\rho^l_i,\\nu^k_i\\) datapoint \\(x_i\\) \\(l\\)-th nearest neighbours \\(X\\) \\(k\\)-th nearest neighbours \\(Y\\), respectively, may lead unequal biases NN density estimation, especially high-dimensional setting. overcome issue, number neighbours \\(l,k\\) chosen way render \\(\\rho^l_i,\\nu^k_i\\) comparable, taking largest possible number neighbours \\(l_i,k_i\\) smaller \\(\\delta_i:=\\max(\\rho^1_i,\\nu^1_i)\\). Since bias reduction explicitly uses samples X Y, one-sample estimation possible using method. Reference: Wang, Kulkarni Verdú, \"Divergence Estimation Multidimensional Densities Via k-Nearest-Neighbor Distances\", IEEE Transactions Information Theory, Vol. 55, . 5 (2009). DOI: https://doi.org/10.1109/TIT.2009.2016060","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_brnn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bias-reduced generalized k-nearest-neighbour KL divergence estimation — kld_est_brnn","text":"","code":"# KL-D between one or two samples from 1-D Gaussians: X <- rnorm(100) Y <- rnorm(100, mean = 1, sd = 2) q <- function(x) dnorm(x, mean = 1, sd =2) kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2) #> [1] 0.4431472 kld_est_nn(X, Y) #> [1] 0.8327817 kld_est_nn(X, q = q) #> [1] 0.4612684 kld_est_nn(X, Y, k = 5) #> [1] 0.5574211 kld_est_nn(X, q = q, k = 5) #> [1] 0.4804156 kld_est_brnn(X, Y) #> [1] 0.7859835   # KL-D between two samples from 2-D Gaussians: X1 <- rnorm(100) X2 <- rnorm(100) Y1 <- rnorm(100) Y2 <- Y1 + rnorm(100) X <- cbind(X1,X2) Y <- cbind(Y1,Y2) kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),              mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2)) #> [1] 0.5 kld_est_nn(X, Y) #> [1] 0.276804 kld_est_nn(X, Y, k = 5) #> [1] 0.1749537 kld_est_brnn(X, Y) #> [1] 0.2092749"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_discrete.html","id":null,"dir":"Reference","previous_headings":"","what":"Plug-in KL divergence estimator for samples from discrete distributions — kld_est_discrete","title":"Plug-in KL divergence estimator for samples from discrete distributions — kld_est_discrete","text":"Plug-KL divergence estimator samples discrete distributions","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_discrete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plug-in KL divergence estimator for samples from discrete distributions — kld_est_discrete","text":"","code":"kld_est_discrete(X, Y = NULL, q = NULL)"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_discrete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plug-in KL divergence estimator for samples from discrete distributions — kld_est_discrete","text":"X, Y n--d m--d matrices data frames, representing n samples true discrete distribution \\(P\\) m samples approximate discrete distribution \\(Q\\), d dimensions. Vector input treated column matrix. Argument Y can omitted argument q given (see ). q probability mass function approximate distribution \\(Q\\). Currently, one-sample problem implemented d=1.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_discrete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plug-in KL divergence estimator for samples from discrete distributions — kld_est_discrete","text":"scalar, estimated Kullback-Leibler divergence \\(\\hat D_{KL}(P||Q)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_discrete.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plug-in KL divergence estimator for samples from discrete distributions — kld_est_discrete","text":"","code":"# 1D example, two samples X <- c(rep('M',5),rep('F',5)) Y <- c(rep('M',6),rep('F',4)) kld_est_discrete(X, Y) #> [1] 0.020411  # 1D example, one sample X <- c(rep(0,4),rep(1,6)) q <- function(x) dbinom(x, size = 1, prob = 0.5) kld_est_discrete(X, q = q) #> [1] 0.02013551"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde.html","id":null,"dir":"Reference","previous_headings":"","what":"Kernel density-based Kullback-Leibler divergence estimation in any dimension — kld_est_kde","title":"Kernel density-based Kullback-Leibler divergence estimation in any dimension — kld_est_kde","text":"Disclaimer: function use binning /fast Fourier transform hence, extremely slow even moderate datasets. reason, exported currently.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kernel density-based Kullback-Leibler divergence estimation in any dimension — kld_est_kde","text":"","code":"kld_est_kde(X, Y, hX = NULL, hY = NULL, rule = c(\"Silverman\", \"Scott\"))"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kernel density-based Kullback-Leibler divergence estimation in any dimension — kld_est_kde","text":"X, Y n--d m--d matrices, representing n samples true distribution \\(P\\) m samples approximate distribution \\(Q\\), d dimensions. Vector input treated column matrix. hX, hY Positive scalars length d vectors, representing bandwidth parameters (possibly different component) density estimates \\(P\\) \\(Q\\), respectively. unspecified, heurestic specified via rule argument used. rule heuristic computing arguments hX /hY. default \"silverman\" Silverman's rule $$h_i = \\sigma_i\\left(\\frac{4}{(2+d)n}\\right)^{1/(d+4)}.$$ alternative, Scott's rule \"scott\" can used, $$h_i = \\frac{\\sigma_i}{n^{1/(d+4)}}.$$","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Kernel density-based Kullback-Leibler divergence estimation in any dimension — kld_est_kde","text":"scalar, estimated Kullback-Leibler divergence \\(\\hat D_{KL}(P||Q)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Kernel density-based Kullback-Leibler divergence estimation in any dimension — kld_est_kde","text":"estimation method approximates densities unknown distributions \\(P\\) \\(Q\\) kernel density estimates, using sample size- dimension-dependent bandwidth parameter Gaussian kernel. works number dimensions slow.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Kernel density-based Kullback-Leibler divergence estimation in any dimension — kld_est_kde","text":"","code":"# KL-D between two samples from 1-D Gaussians: X <- rnorm(100) Y <- rnorm(100, mean = 1, sd = 2) kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2) #> [1] 0.4431472 kld_est_kde1(X, Y) #> [1] 0.4851639 kld_est_nn(X, Y) #> [1] 0.7679835 kld_est_brnn(X, Y) #> [1] 0.7406705  # KL-D between two samples from 2-D Gaussians: X1 <- rnorm(100) X2 <- rnorm(100) Y1 <- rnorm(100) Y2 <- Y1 + rnorm(100) X <- cbind(X1,X2) Y <- cbind(Y1,Y2) kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),              mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2)) #> [1] 0.5 kld_est_kde2(X, Y) #> [1] 0.26987 kld_est_nn(X, Y) #> [1] 0.6152511 kld_est_brnn(X, Y) #> [1] 0.6349888"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde1.html","id":null,"dir":"Reference","previous_headings":"","what":"1-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde1","title":"1-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde1","text":"estimation method approximates densities unknown distributions \\(P\\) \\(Q\\) kernel density estimate using function 'density' package 'stats'. two-sample, one-sample problem implemented.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"1-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde1","text":"","code":"kld_est_kde1(X, Y, MC = FALSE, ...)"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"1-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde1","text":"X, Y Numeric vectors single-column matrices, representing samples true distribution \\(P\\) approximate distribution \\(Q\\), respectively. MC boolean: use Monte Carlo approximation instead numerical integration via trapezoidal rule (default: FALSE)? ... parameters passed stats::density (e.g., argument bw)","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"1-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde1","text":"scalar, estimated Kullback-Leibler divergence \\(\\hat D_{KL}(P||Q)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde1.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"1-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde1","text":"","code":"# KL-D between two samples from 1D Gaussians: X <- rnorm(100) Y <- rnorm(100, mean = 1, sd = 2) kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2) #> [1] 0.4431472 kld_est_kde1(X,Y) #> [1] 0.4726456 kld_est_kde1(X,Y, MC = TRUE) #> [1] 0.5291433"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde2.html","id":null,"dir":"Reference","previous_headings":"","what":"2-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde2","title":"2-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde2","text":"estimation method approximates densities unknown bivariate distributions \\(P\\) \\(Q\\) kernel density estimates using function 'bkde' package 'KernSmooth'. 'KernSmooth' installed, message issued (much) slower function 'kld_est_kde' used instead.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"2-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde2","text":"","code":"kld_est_kde2(   X,   Y,   MC = FALSE,   hX = NULL,   hY = NULL,   rule = c(\"Silverman\", \"Scott\"),   eps = 1e-05 )"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"2-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde2","text":"X, Y n--2 m--2 matrices, representing n samples bivariate true distribution \\(P\\) m samples approximate distribution \\(Q\\), respectively. MC boolean: use Monte Carlo approximation instead numerical integration via trapezoidal rule (default: FALSE)? Currently, option implemented, .e. value TRUE results error. hX, hY Bandwidths kernel density estimates \\(P\\) \\(Q\\), respectively. default NULL means determined argument rule. rule heuristic derive parameters hX hY, default \"Silverman\", means $$h_i = \\sigma_i\\left(\\frac{4}{(2+d)n}\\right)^{1/(d+4)}.$$ eps nonnegative scalar; eps > 0, \\(Q\\) estimated mixture kernel density estimate uniform distribution computational grid. weight uniform component eps times maximum density estimate \\(Q\\). increases robustness estimator expense additional bias. Defaults eps = 1e-5.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"2-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde2","text":"scalar, estimated Kullback-Leibler divergence \\(\\hat D_{KL}(P||Q)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"2-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde2","text":"","code":"# KL-D between two samples from 2-D Gaussians: X1 <- rnorm(1000) X2 <- rnorm(1000) Y1 <- rnorm(1000) Y2 <- Y1 + rnorm(1000) X <- cbind(X1,X2) Y <- cbind(Y1,Y2) kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),              mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2)) #> [1] 0.5 kld_est_kde2(X,Y) #> [1] 0.3480714 # kld_est_kde2(X,Y, MC = TRUE)"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_nn.html","id":null,"dir":"Reference","previous_headings":"","what":"k-nearest neighbour KL divergence estimator — kld_est_nn","title":"k-nearest neighbour KL divergence estimator — kld_est_nn","text":"function estimates Kullback-Leibler divergence \\(D_{KL}(P||Q)\\) two continuous distributions \\(P\\) \\(Q\\) using nearest-neighbour (NN) density estimation Monte Carlo approximation \\(D_{KL}(P||Q)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_nn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"k-nearest neighbour KL divergence estimator — kld_est_nn","text":"","code":"kld_est_nn(X, Y = NULL, q = NULL, k = 1L, eps = 0, log.q = FALSE)"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_nn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"k-nearest neighbour KL divergence estimator — kld_est_nn","text":"X, Y n--d m--d matrices, representing n samples true distribution \\(P\\) m samples approximate distribution \\(Q\\), d dimensions. Vector input treated column matrix. Y can left blank q specified (see ). q density function approximate distribution \\(Q\\). Either Y q must specified. k number nearest neighbours consider NN density estimation. Larger values k generally increase bias, decrease variance estimator. Defaults k = 1. eps Error bound nearest neighbour search. value eps = 0 (default) implies exact nearest neighbour search, eps > 0 approximate nearest neighbours sought, may somewhat faster high-dimensional problems. log.q TRUE, function q log-density rather density approximate distribution \\(Q\\) (default: log.q = FALSE).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_nn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"k-nearest neighbour KL divergence estimator — kld_est_nn","text":"scalar, estimated Kullback-Leibler divergence \\(\\hat D_{KL}(P||Q)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_nn.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"k-nearest neighbour KL divergence estimator — kld_est_nn","text":"Input estimation sample X \\(P\\) either density function q \\(Q\\) (one-sample problem) sample Y \\(Q\\) (two-sample problem). two-sample problem, estimator Eq.(5) Wang et al. (2009). one-sample problem, asymptotic bias (expectation Gamma distribution) substracted, see Pérez-Cruz (2008), Eq.(18). References: Wang, Kulkarni Verdú, \"Divergence Estimation Multidimensional Densities Via k-Nearest-Neighbor Distances\", IEEE Transactions Information Theory, Vol. 55, . 5 (2009). Pérez-Cruz, \"Kullback-Leibler Divergence Estimation Continuous Distributions\", IEEE International Symposium Information Theory (2008).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_nn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"k-nearest neighbour KL divergence estimator — kld_est_nn","text":"","code":"# KL-D between one or two samples from 1-D Gaussians: X <- rnorm(100) Y <- rnorm(100, mean = 1, sd = 2) q <- function(x) dnorm(x, mean = 1, sd =2) kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2) #> [1] 0.4431472 kld_est_nn(X, Y) #> [1] 0.1376493 kld_est_nn(X, q = q) #> [1] 0.3769996 kld_est_nn(X, Y, k = 5) #> [1] 0.3335666 kld_est_nn(X, q = q, k = 5) #> [1] 0.457862 kld_est_brnn(X, Y) #> [1] 0.1680454   # KL-D between two samples from 2-D Gaussians: X1 <- rnorm(100) X2 <- rnorm(100) Y1 <- rnorm(100) Y2 <- Y1 + rnorm(100) X <- cbind(X1,X2) Y <- cbind(Y1,Y2) kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),              mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2)) #> [1] 0.5 kld_est_nn(X, Y) #> [1] 0.007660125 kld_est_nn(X, Y, k = 5) #> [1] 0.2031239 kld_est_brnn(X, Y) #> [1] 0.1338997"},{"path":"https://niklhart.github.io/kldest/reference/kld_exponential.html","id":null,"dir":"Reference","previous_headings":"","what":"Analytical KL divergence for two univariate exponential distributions — kld_exponential","title":"Analytical KL divergence for two univariate exponential distributions — kld_exponential","text":"function computes \\(D_{KL}(p||q)\\), \\(p\\sim \\text{Exp}(\\lambda_1)\\) \\(q\\sim \\text{Exp}(\\lambda_2)\\), rate parametrization.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_exponential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analytical KL divergence for two univariate exponential distributions — kld_exponential","text":"","code":"kld_exponential(lambda1, lambda2)"},{"path":"https://niklhart.github.io/kldest/reference/kld_exponential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analytical KL divergence for two univariate exponential distributions — kld_exponential","text":"lambda1 scalar (rate parameter true exponential distribution) lambda2 scalar (rate parameter approximate exponential distribution)","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_exponential.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analytical KL divergence for two univariate exponential distributions — kld_exponential","text":"scalar (Kullback-Leibler divergence)","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_exponential.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analytical KL divergence for two univariate exponential distributions — kld_exponential","text":"","code":"kld_exponential(lambda1 = 1, lambda2 = 2) #> [1] 0.3068528"},{"path":"https://niklhart.github.io/kldest/reference/kld_gaussian.html","id":null,"dir":"Reference","previous_headings":"","what":"Analytical KL divergence for two uni- or multivariate Gaussian distributions — kld_gaussian","title":"Analytical KL divergence for two uni- or multivariate Gaussian distributions — kld_gaussian","text":"function computes \\(D_{KL}(p||q)\\), \\(p\\sim \\mathcal{N}(\\mu_1,\\Sigma_1)\\) \\(q\\sim \\mathcal{N}(\\mu_2,\\Sigma_2)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_gaussian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analytical KL divergence for two uni- or multivariate Gaussian distributions — kld_gaussian","text":"","code":"kld_gaussian(mu1, sigma1, mu2, sigma2)"},{"path":"https://niklhart.github.io/kldest/reference/kld_gaussian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analytical KL divergence for two uni- or multivariate Gaussian distributions — kld_gaussian","text":"mu1 numeric vector (mean true Gaussian) sigma1 s.p.d. matrix (Covariance matrix true Gaussian) mu2 numeric vector (mean approximate Gaussian) sigma2 s.p.d. matrix  (Covariance matrix approximate Gaussian)","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_gaussian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analytical KL divergence for two uni- or multivariate Gaussian distributions — kld_gaussian","text":"scalar (Kullback-Leibler divergence)","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_gaussian.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analytical KL divergence for two uni- or multivariate Gaussian distributions — kld_gaussian","text":"","code":"kld_gaussian(mu1 = 1, sigma1 = 1, mu2 = 1, sigma2 = 2^2) #> [1] 0.3181472 kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),                 mu2 = rep(1,2), sigma2 = matrix(c(1,0.5,0.5,1), nrow = 2)) #> [1] 0.856159"},{"path":"https://niklhart.github.io/kldest/reference/kld_uniform.html","id":null,"dir":"Reference","previous_headings":"","what":"Analytical KL divergence for two uniform distributions — kld_uniform","title":"Analytical KL divergence for two uniform distributions — kld_uniform","text":"function computes \\(D_{KL}(p||q)\\), \\(p\\sim \\text{U}(a_1,b_1)\\) \\(q\\sim \\text{U}(a_2,b_2)\\), \\(a_2<a_1<b_1<b_2\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_uniform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analytical KL divergence for two uniform distributions — kld_uniform","text":"","code":"kld_uniform(a1, b1, a2, b2)"},{"path":"https://niklhart.github.io/kldest/reference/kld_uniform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analytical KL divergence for two uniform distributions — kld_uniform","text":"a1, b1 Range true uniform distribution a2, b2 Range approximate uniform distribution","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_uniform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analytical KL divergence for two uniform distributions — kld_uniform","text":"scalar (Kullback-Leibler divergence)","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_uniform.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analytical KL divergence for two uniform distributions — kld_uniform","text":"","code":"kld_uniform(a1 = 0, b1 = 1, a2 = 0, b2 = 2) #> [1] 0.6931472"},{"path":"https://niklhart.github.io/kldest/reference/kldest-package.html","id":null,"dir":"Reference","previous_headings":"","what":"kldest: Sample-based estimation of Kullback-Leibler divergence — kldest-package","title":"kldest: Sample-based estimation of Kullback-Leibler divergence — kldest-package","text":"collection estimation algorithms Kullback-Leibler divergence two probability distributions, based one two samples, including uncertainty quantification. Distributions can uni- multivariate continous, discrete mixed.","code":""},{"path":[]},{"path":"https://niklhart.github.io/kldest/reference/kldest-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"kldest: Sample-based estimation of Kullback-Leibler divergence — kldest-package","text":"Maintainer: Niklas Hartung niklas.hartung@gmail.com (ORCID) [copyright holder]","code":""},{"path":"https://niklhart.github.io/kldest/reference/mvdnorm.html","id":null,"dir":"Reference","previous_headings":"","what":"Probability density function of multivariate Gaussian distribution — mvdnorm","title":"Probability density function of multivariate Gaussian distribution — mvdnorm","text":"Probability density function multivariate Gaussian distribution","code":""},{"path":"https://niklhart.github.io/kldest/reference/mvdnorm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Probability density function of multivariate Gaussian distribution — mvdnorm","text":"","code":"mvdnorm(x, mu, Sigma)"},{"path":"https://niklhart.github.io/kldest/reference/mvdnorm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Probability density function of multivariate Gaussian distribution — mvdnorm","text":"x vector length d Gaussian density evaluated. mu vector length d, mean Gaussian distribution. Sigma d--d matrix, covariance matrix Gaussian distribution.","code":""},{"path":"https://niklhart.github.io/kldest/reference/mvdnorm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Probability density function of multivariate Gaussian distribution — mvdnorm","text":"probability density \\(N(\\mu,\\Sigma)\\) evaluated x.","code":""},{"path":"https://niklhart.github.io/kldest/reference/mvdnorm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Probability density function of multivariate Gaussian distribution — mvdnorm","text":"","code":"# 1D example mvdnorm(x = 2, mu = 1, Sigma = 2) #> [1] 0.2196956 dnorm(x = 2, mean = 1, sd = sqrt(2)) #> [1] 0.2196956 # Independent 2D example mvdnorm(x = c(2,2), mu = c(1,1), Sigma = diag(1:2)) #> [1] 0.05315991 prod(dnorm(x = c(2,2), mean = c(1,1), sd = sqrt(1:2))) #> [1] 0.05315991 # Correlated 2D example mvdnorm(x = c(2,2), mu = c(1,1), Sigma = matrix(c(2,1,1,2),nrow=2)) #> [1] 0.06584074"},{"path":"https://niklhart.github.io/kldest/reference/to_uniform_scale.html","id":null,"dir":"Reference","previous_headings":"","what":"Transform samples to uniform scale — to_uniform_scale","title":"Transform samples to uniform scale — to_uniform_scale","text":"Since Kullback-Leibler divergence scale-invariant, sample-based approximations can computed conveniently chosen scale. helper functions transforms variable way marginal distributions joint dataset \\((X,Y)\\) uniform. way, scales different variables rendered comparable, idea better performance neighbour-based methods situation.","code":""},{"path":"https://niklhart.github.io/kldest/reference/to_uniform_scale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transform samples to uniform scale — to_uniform_scale","text":"","code":"to_uniform_scale(X, Y)"},{"path":"https://niklhart.github.io/kldest/reference/to_uniform_scale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transform samples to uniform scale — to_uniform_scale","text":"X, Y n--d m--d matrices, representing n samples true distribution \\(P\\) m samples approximate distribution \\(Q\\), d dimensions. Vector input treated column matrix. Y can left blank q specified (see ).","code":""},{"path":"https://niklhart.github.io/kldest/reference/to_uniform_scale.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transform samples to uniform scale — to_uniform_scale","text":"list fields X Y, containing transformed samples.","code":""},{"path":"https://niklhart.github.io/kldest/reference/to_uniform_scale.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transform samples to uniform scale — to_uniform_scale","text":"","code":"# 2D example n <- 10L X <- cbind(rnorm(n, mean = 0, sd = 3),            rnorm(n, mean = 1, sd = 2)) Y <- cbind(rnorm(n, mean = 1, sd = 2),            rnorm(n, mean = 0, sd = 2)) to_uniform_scale(X, Y) #> $X #>       [,1] [,2] #>  [1,] 0.30 0.95 #>  [2,] 1.00 0.30 #>  [3,] 0.55 0.65 #>  [4,] 0.50 0.75 #>  [5,] 0.90 0.85 #>  [6,] 0.60 0.60 #>  [7,] 0.95 0.80 #>  [8,] 0.85 0.50 #>  [9,] 0.15 0.55 #> [10,] 0.35 0.90 #>  #> $Y #>       [,1] [,2] #>  [1,] 0.05 0.25 #>  [2,] 0.75 0.70 #>  [3,] 0.25 0.10 #>  [4,] 0.10 0.05 #>  [5,] 0.80 0.45 #>  [6,] 0.45 0.20 #>  [7,] 0.70 0.35 #>  [8,] 0.40 0.15 #>  [9,] 0.65 0.40 #> [10,] 0.20 1.00 #>"},{"path":"https://niklhart.github.io/kldest/reference/tr.html","id":null,"dir":"Reference","previous_headings":"","what":"Matrix trace operator — tr","title":"Matrix trace operator — tr","text":"Matrix trace operator","code":""},{"path":"https://niklhart.github.io/kldest/reference/tr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Matrix trace operator — tr","text":"","code":"tr(M)"},{"path":"https://niklhart.github.io/kldest/reference/tr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matrix trace operator — tr","text":"M square matrix","code":""},{"path":"https://niklhart.github.io/kldest/reference/tr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matrix trace operator — tr","text":"matrix trace (scalar)","code":""},{"path":"https://niklhart.github.io/kldest/reference/trapz.html","id":null,"dir":"Reference","previous_headings":"","what":"Trapezoidal integration in d dimensions — trapz","title":"Trapezoidal integration in d dimensions — trapz","text":"Trapezoidal integration d dimensions","code":""},{"path":"https://niklhart.github.io/kldest/reference/trapz.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trapezoidal integration in d dimensions — trapz","text":"","code":"trapz(h, fx)"},{"path":"https://niklhart.github.io/kldest/reference/trapz.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trapezoidal integration in d dimensions — trapz","text":"h length d numeric vector grid widths. fx d-dimensional array (vector, d=1).","code":""},{"path":"https://niklhart.github.io/kldest/reference/trapz.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trapezoidal integration in d dimensions — trapz","text":"trapezoidal approximation integral.","code":""},{"path":"https://niklhart.github.io/kldest/reference/trapz.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Trapezoidal integration in d dimensions — trapz","text":"","code":"# 1D example trapz(h = 1, fx = 1:10) #> [1] 49.5 # 2D example trapz(h = c(1,1), fx = matrix(1:10, nrow = 2)) #> [1] 22"}]
