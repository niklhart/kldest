[{"path":"https://niklhart.github.io/kldest/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 kldest authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":[]},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-1d.html","id":"distributions-and-kl-d","dir":"Articles","previous_headings":"Specification of simulation scenario","what":"Distributions and KL-D","title":"Algorithm benchmark in 1D","text":"investigate following pairs distributions, analytical KL divergence values known: \\(\\mathcal{N}(0,1)\\) vs. \\(\\mathcal{N}(1,2^2)\\), \\(\\mathcal{U}(1,2)\\) vs. \\(\\mathcal{U}(0,4)\\), \\(\\text{Exp}(1)\\) vs. \\(\\text{Exp}(1/12)\\). Analytical values Kullback-Leibler divergences test cases:","code":"p <- list(     gaussian    = list(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2),     uniform     = list(a1 = 1, b1 = 2, a2 = 0, b2 = 4),     exponential = list(lambda1 = 1, lambda2 = 1/12) ) distributions <- list(     gaussian = list(         samples = function(n, m) {             X <- rnorm(n, mean = p$gaussian$mu1, sd = sqrt(p$gaussian$sigma1))             Y <- rnorm(m, mean = p$gaussian$mu2, sd = sqrt(p$gaussian$sigma2))             list(X = X, Y = Y)         },         kld = do.call(kld_gaussian, p$gaussian)     ),     uniform = list(          samples = function(n, m) {             X <- runif(n, min = p$uniform$a1, max = p$uniform$b1)             Y <- runif(m, min = p$uniform$a2, max = p$uniform$b2)             list(X = X, Y = Y)         },         kld = do.call(kld_uniform, p$uniform)     ),     exponential = list(         samples = function(n, m) {             X <- rexp(n, rate = p$exponential$lambda1)             Y <- rexp(m, rate = p$exponential$lambda2)             list(X = X, Y = Y)         },         kld = do.call(kld_exponential, p$exponential)     ) ) vapply(distributions, function(x) x$kld, 1) #>    gaussian     uniform exponential  #>   0.4431472   1.3862944   1.5682400"},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-1d.html","id":"simulation-scenarios","dir":"Articles","previous_headings":"Specification of simulation scenario","what":"Simulation scenarios","title":"Algorithm benchmark in 1D","text":"distributions specified , samples different sizes drawn, several replicates per distribution sample size.","code":"samplesize <- 10^(2:4) nRep       <- 25L  scenarios <- combinations(     distribution = names(distributions),     sample.size  = samplesize,     replicate    = 1:nRep )"},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-1d.html","id":"algorithms","dir":"Articles","previous_headings":"Specification of simulation scenario","what":"Algorithms","title":"Algorithm benchmark in 1D","text":"consder following algorithms: kernel density estimation numerical integration (dens_int) kernel density estimation Monte Carlo approximation (dens_mc) 1-nearest neighbour density estimation (nn_1) bias-reduced nearest neighbour density estimation (nn_br)","code":"algorithms <- list(     dens_int = function(X, Y) kld_est_kde1(X = X, Y = Y, MC = FALSE),     dens_mc  = function(X, Y) kld_est_kde1(X = X, Y = Y, MC = TRUE),     nn_1     = kld_est_nn,     nn_br = function(X, Y) kld_est_brnn(X = X, Y = Y, warn.max.k = FALSE) ) nAlgo   <- length(algorithms)"},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-1d.html","id":"run-the-simulation-study","dir":"Articles","previous_headings":"","what":"Run the simulation study","title":"Algorithm benchmark in 1D","text":"Post-processing: combine scenarios, kldiv1d runtime single data frame","code":"# allocating results matrices nscenario  <- nrow(scenarios) runtime <- kldiv1d <- matrix(nrow = nscenario,                               ncol = nAlgo,                               dimnames = list(NULL, names(algorithms)))  for (i in 1:nscenario) {      dist <- scenarios$distribution[i]     n    <- scenarios$sample.size[i]          samples <- distributions[[dist]]$sample(n = n, m = n)     X <- samples$X     Y <- samples$Y          # different algorithms are evaluated on the same samples     for (j in 1:nAlgo) {         algo <- algorithms[[j]]         start_time <- Sys.time()         kldiv1d[i,j] <- algo(X, Y)         end_time <- Sys.time()         runtime[i,j] <- end_time - start_time     } } tmp1 <- cbind(scenarios, kldiv1d) |> melt(measure.vars = names(algorithms),                                           value.name = \"kld\",                                           variable.name = \"algorithm\")  tmp2 <- cbind(scenarios, runtime) |> melt(measure.vars = names(algorithms),                                           value.name = \"runtime\",                                           variable.name = \"algorithm\")  results <- merge(tmp1,tmp2) results$sample.size <- as.factor(results$sample.size) rm(tmp1,tmp2)"},{"path":[]},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-1d.html","id":"accuracy-of-kl-divergence-estimators","dir":"Articles","previous_headings":"Results","what":"Accuracy of KL divergence estimators","title":"Algorithm benchmark in 1D","text":"\\(\\Rightarrow\\) estimators converge towards true KL divergence (black solid line). Kernel density-based estimators generally lower variance nearest neighbour-based estimators, show finite sample bias, especially asymmetric exponential distribution. difference 1-nearest neighbour bias-reduced k-nearest neighbour methods terms accuracy.","code":"ggplot(results, aes(x=sample.size, y=kld, color=algorithm)) +      geom_jitter(position=position_dodge(.5)) +      facet_wrap(\"distribution\", scales = \"free_y\") +     geom_hline(data = data.frame(distribution = names(distributions), kldtrue = vapply(distributions, function(x) x$kld,1)),                 aes(yintercept = kldtrue))+     xlab(\"Sample sizes\") + ylab(\"KL divergence estimate\") + ggtitle(\"Accuracy of different algorithms\") #> Warning: Removed 4 rows containing missing values (`geom_point()`)."},{"path":"https://niklhart.github.io/kldest/articles/algorithm-benchmark-1d.html","id":"runtime-of-kl-divergence-estimators","dir":"Articles","previous_headings":"Results","what":"Runtime of KL divergence estimators","title":"Algorithm benchmark in 1D","text":"\\(\\Rightarrow\\) Kernel density-based estimators, use stats::density, generally fastest (except small sample sizes). investigated methods scale approximately linearly sample size, due use fast Fourier transform kernel density estimation use kd-tree nearest neighbours search. bias-reduced nearest neighbour estimator nn_br approximately 1 order magnitude slower 1-nearest neighbour estimator nn_1, without offering additional accuracy 1-D examples. extra effort starts pay higher-dimensional problems.","code":"ggplot(results, aes(x=sample.size, y=runtime, color=algorithm)) +      scale_y_log10() +      geom_jitter(position=position_dodge(.5)) +      facet_wrap(\"distribution\", scales = \"free_y\") +     xlab(\"Sample sizes\") + ylab(\"Runtime [sec]\") + ggtitle(\"Runtime of different algorithms\")"},{"path":"https://niklhart.github.io/kldest/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Niklas Hartung. Author, maintainer.","code":""},{"path":"https://niklhart.github.io/kldest/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Hartung N (2023). kldest: Sample-based estimation Kullback-Leibler divergence. R package version 0.1.0, https://niklhart.github.io/kldest/.","code":"@Manual{,   title = {kldest: Sample-based estimation of Kullback-Leibler divergence},   author = {Niklas Hartung},   year = {2023},   note = {R package version 0.1.0},   url = {https://niklhart.github.io/kldest/}, }"},{"path":"https://niklhart.github.io/kldest/index.html","id":"kldest","dir":"","previous_headings":"","what":"Sample-based estimation of Kullback-Leibler divergence","title":"Sample-based estimation of Kullback-Leibler divergence","text":"goal kldest estimate Kullback-Leibler (KL) divergence DKL(P||Q) two probability distributions P Q based : sample x1, ..., xn P probability density q Q, samples x1, ..., xn P y1, ..., ym Q. distributions P Q may uni- multivariate, may discrete, continuous mixed discrete/continuous. Different estimation algorithms provided, either based nearest neighbour density estimation kernel density estimation. Confidence intervals KL divergence can also computed, either via bootstrapping subsampling.","code":""},{"path":"https://niklhart.github.io/kldest/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Sample-based estimation of Kullback-Leibler divergence","text":"can install development version kldest GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"niklhart/kldest\")"},{"path":"https://niklhart.github.io/kldest/index.html","id":"minimal-example-for-nearest-neighbour-density-estimation","dir":"","previous_headings":"","what":"Minimal example for nearest neighbour density estimation","title":"Sample-based estimation of Kullback-Leibler divergence","text":"KL divergence estimation based nearest neighbour density estimates flexible approach.","code":"library(kldest)"},{"path":"https://niklhart.github.io/kldest/index.html","id":"kl-divergence-1-d-gaussians","dir":"","previous_headings":"Minimal example for nearest neighbour density estimation","what":"KL divergence (1-D Gaussians)","title":"Sample-based estimation of Kullback-Leibler divergence","text":"Two Samples Gaussians One sample Gaussian Gaussian density Uncertainty quantification","code":"X <- rnorm(100) Y <- rnorm(100, mean = 1, sd = 2) kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2) #> [1] 0.4431472 kld_est_nn(X, Y) #> [1] 0.6154485 q <- function(x) dnorm(x, mean = 1, sd =2) kld_est_nn(X, q = q) #> [1] 0.3971703 kld_ci_subsampling(X, q = q)$ci #>       2.5%      97.5%  #> 0.03345729 0.62108311"},{"path":"https://niklhart.github.io/kldest/index.html","id":"kl-d-between-two-samples-from-2-d-gaussians","dir":"","previous_headings":"Minimal example for nearest neighbour density estimation","what":"KL-D between two samples from 2-D Gaussians","title":"Sample-based estimation of Kullback-Leibler divergence","text":"","code":"X1 <- rnorm(100) X2 <- rnorm(100) Y1 <- rnorm(100) Y2 <- Y1 + rnorm(100) X <- cbind(X1,X2) Y <- cbind(Y1,Y2)  # True KL divergence kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),              mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2)) #> [1] 0.5  # KL divergence estimate kld_est_nn(X, Y) #> [1] 0.2804118"},{"path":"https://niklhart.github.io/kldest/reference/combinations.html","id":null,"dir":"Reference","previous_headings":"","what":"Combinations of input arguments — combinations","title":"Combinations of input arguments — combinations","text":"Combinations input arguments","code":""},{"path":"https://niklhart.github.io/kldest/reference/combinations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combinations of input arguments — combinations","text":"","code":"combinations(...)"},{"path":"https://niklhart.github.io/kldest/reference/combinations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combinations of input arguments — combinations","text":"... number atomic vectors.","code":""},{"path":"https://niklhart.github.io/kldest/reference/combinations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combinations of input arguments — combinations","text":"data frame columns named inputs, containing input combinations.","code":""},{"path":"https://niklhart.github.io/kldest/reference/combinations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Combinations of input arguments — combinations","text":"","code":"combinations(a = 1:2, b = letters[1:3], c = LETTERS[1:2]) #>    a b c #> 1  1 a A #> 2  2 a A #> 3  1 b A #> 4  2 b A #> 5  1 c A #> 6  2 c A #> 7  1 a B #> 8  2 a B #> 9  1 b B #> 10 2 b B #> 11 1 c B #> 12 2 c B"},{"path":"https://niklhart.github.io/kldest/reference/constDiagMatrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Constant plus diagonal matrix — constDiagMatrix","title":"Constant plus diagonal matrix — constDiagMatrix","text":"Specify matrix constant values diagonal -diagonals. matrices can used vary degree dependency covariate matrices, example evaluating accuracy KL-divergence estimation algorithms.","code":""},{"path":"https://niklhart.github.io/kldest/reference/constDiagMatrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Constant plus diagonal matrix — constDiagMatrix","text":"","code":"constDiagMatrix(dim = 1, diag = 1, offDiag = 0)"},{"path":"https://niklhart.github.io/kldest/reference/constDiagMatrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Constant plus diagonal matrix — constDiagMatrix","text":"dim Dimension diag Value diagonal offDiag Value -diagonals","code":""},{"path":"https://niklhart.github.io/kldest/reference/constDiagMatrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Constant plus diagonal matrix — constDiagMatrix","text":"dim--dim matrix","code":""},{"path":"https://niklhart.github.io/kldest/reference/constDiagMatrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Constant plus diagonal matrix — constDiagMatrix","text":"","code":"constDiagMatrix(dim = 3, diag = 1, offDiag = 0.9) #>      [,1] [,2] [,3] #> [1,]  1.0  0.9  0.9 #> [2,]  0.9  1.0  0.9 #> [3,]  0.9  0.9  1.0"},{"path":"https://niklhart.github.io/kldest/reference/is_two_sample.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect if a one- or two-sample problem is specified — is_two_sample","title":"Detect if a one- or two-sample problem is specified — is_two_sample","text":"Detect one- two-sample problem specified","code":""},{"path":"https://niklhart.github.io/kldest/reference/is_two_sample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect if a one- or two-sample problem is specified — is_two_sample","text":"","code":"is_two_sample(Y, q)"},{"path":"https://niklhart.github.io/kldest/reference/is_two_sample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect if a one- or two-sample problem is specified — is_two_sample","text":"Y vector, matrix, data frame NULL q function NULL.","code":""},{"path":"https://niklhart.github.io/kldest/reference/is_two_sample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect if a one- or two-sample problem is specified — is_two_sample","text":"TRUE two-sample problem (.e., Y non-null q = NULL) FALSE one-sample problem (.e., Y = NULL q non-null).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_bootstrap.html","id":null,"dir":"Reference","previous_headings":"","what":"Uncertainty of KL divergence estimate using Efron's bootstrap. — kld_ci_bootstrap","title":"Uncertainty of KL divergence estimate using Efron's bootstrap. — kld_ci_bootstrap","text":"function computes confidence interval KL divergence based Efron's bootstrap. approach works kernel density-based estimators since nearest neighbour-based estimators deal ties produced sampling replacement.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_bootstrap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Uncertainty of KL divergence estimate using Efron's bootstrap. — kld_ci_bootstrap","text":"","code":"kld_ci_bootstrap(X, Y, estimator = kld_est_kde1, B = 500L, alpha = 0.05)"},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_bootstrap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Uncertainty of KL divergence estimate using Efron's bootstrap. — kld_ci_bootstrap","text":"X, Y n--d m--d matrices, representing n samples true distribution \\(P\\) m samples approximate distribution \\(Q\\), d dimensions. Vector input treated column matrix. estimator function expecting two inputs X Y, Kullback-Leibler divergence estimation method. Defaults kld_est_kde1, can deal one-dimensional two-sample problems (.e., d = 1 q = NULL). B Number bootstrap replicates (default: 500), larger, accurate, also computationally expensive. alpha Error level, defaults 0.05.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_bootstrap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Uncertainty of KL divergence estimate using Efron's bootstrap. — kld_ci_bootstrap","text":"list fields \"est\" (estimated KL divergence), \"boot\" (length B numeric vector KL divergence estimates bootstrap samples), \"ci\" (length 2 vector containing lower upper limits estimated confidence interval).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_bootstrap.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Uncertainty of KL divergence estimate using Efron's bootstrap. — kld_ci_bootstrap","text":"Reference: Efron, \"Bootstrap Methods: Another Look Jackknife\", Annals Statistics, Vol. 7, . 1 (1979).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_bootstrap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Uncertainty of KL divergence estimate using Efron's bootstrap. — kld_ci_bootstrap","text":"","code":"# 1D Gaussian, two samples X <- rnorm(100) Y <- rnorm(100, mean = 1, sd = 2) kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2) #> [1] 0.4431472 kld_est_kde1(X, Y) #> [1] 0.4742318 kld_ci_bootstrap(X, Y) #> $est #> [1] 0.4742318 #>  #> $boot #>   [1] 0.7268792 0.5701810 0.5142156 0.6963252 0.6168621 0.4534190 0.5319263 #>   [8] 0.4264821 0.4965600 0.5749274 0.4914559 0.4122523 0.4750368 0.5908466 #>  [15] 0.5896513 0.4535925 0.4123061 0.5467812 0.6252856 0.6587475 0.5592077 #>  [22] 0.4706688 0.2847413 0.4706661 0.4487507 0.4041206 0.4949388 0.5564426 #>  [29] 0.4238606 0.3980932 0.5380378 0.5374383 0.6772957 0.6110023 0.5029197 #>  [36] 0.6421515 0.5909363 0.5812507 0.6213512 0.5244721 0.4464640 0.4105032 #>  [43] 0.6129854 0.5988591 0.6353158 0.4846607 0.6152768 0.4550395 0.6114732 #>  [50] 0.4725927 0.4609120 0.6091877 0.7168709 0.4758790 0.7657868 0.5101467 #>  [57] 0.6075965 0.3212671 0.4809555 0.4590775 0.3529370 0.5860860 0.5384212 #>  [64] 0.6515393 0.4894546 0.4333627 0.5883312 0.4648523 0.4843993 0.7510941 #>  [71] 0.4955555 0.3581731 0.5486965 0.5831307 0.5691179 0.7262431 0.5355411 #>  [78] 0.6263840 0.4041037 0.4743077 0.7008684 0.6214790 0.5704322 0.4012176 #>  [85] 0.5613289 0.6229310 0.5933526 0.3241398 0.5379907 0.4643824 0.4714580 #>  [92] 0.4416337 0.4911904 0.5588434 0.4148489 0.3661578 0.3915477 0.5350162 #>  [99] 0.4617430 0.4494433 0.5175862 0.5805982 0.4634924 0.6998121 0.4972940 #> [106] 0.4153063 0.2909862 0.5469323 0.6121560 0.5000731 0.4162647 0.5665244 #> [113] 0.4157386 0.5003498 0.5216893 0.5043520 0.5259336 0.4374349 0.5905698 #> [120] 0.6290429 0.4045211 0.6541578 0.4319214 0.5673303 0.5538572 0.6297819 #> [127] 0.4926927 0.3790505 0.3687178 0.5054838 0.6217428 0.4490602 0.4629686 #> [134] 0.4210480 0.4977018 0.5490171 0.5256610 0.5323598 0.6077392 0.5077406 #> [141] 0.5115820 0.6129213 0.5661409 0.3343203 0.4102714 0.4851036 0.4211769 #> [148] 0.5550966 0.4821337 0.5828197 0.6780051 0.4119590 0.3704688 0.4484376 #> [155] 0.3419939 0.5749320 0.5156625 0.4174928 0.4712343 0.6325820 0.3863714 #> [162] 0.4151049 0.6033254 0.5842202 0.5069127 0.6483842 0.4173760 0.4264816 #> [169] 0.6552875 0.4006958 0.4057597 0.7859721 0.5321690 0.7169424 0.4349792 #> [176] 0.5288682 0.5697059 0.4811072 0.4762157 0.5616735 0.5760839 0.5494225 #> [183] 0.4078381 0.5106791 0.5492321 0.5364980 0.3631797 0.5220954 0.6980757 #> [190] 0.3806673 0.5525996 0.4925890 0.6393960 0.5300620 0.5001994 0.5941849 #> [197] 0.3818391 0.6605101 0.4988376 0.6294051 0.6029637 0.6549538 0.6465341 #> [204] 0.6269271 0.4906094 0.4737846 0.4678744 0.4319913 0.7767213 0.5533653 #> [211] 0.5066803 0.5062953 0.6890323 0.6595922 0.3829071 0.6510748 0.4110026 #> [218] 0.5091385 0.4636898 0.3607468 0.4798451 0.6983867 0.3706519 0.4418332 #> [225] 0.4694361 0.5246716 0.5805739 0.4525727 0.4271134 0.4060185 0.4144180 #> [232] 0.5363131 0.5385253 0.6118266 0.6302383 0.6091166 0.5053635 0.4578109 #> [239] 0.5564455 0.7313536 0.3461167 0.3839021 0.4652478 0.5631743 0.4705506 #> [246] 0.3546107 0.3451336 0.5089301 0.4348207 0.4986162 0.3626558 0.5516112 #> [253] 0.5660044 0.3828654 0.4797113 0.5783817 0.5619527 0.5413491 0.5908250 #> [260] 0.5020984 0.4681087 0.4563402 0.4175474 0.3363384 0.5908963 0.5061498 #> [267] 0.3965401 0.6363310 0.5192099 0.5914666 0.7287171 0.5654966 0.5830175 #> [274] 0.5338142 0.4875757 0.5287558 0.4234872 0.7124835 0.5385809 0.5020249 #> [281] 0.4982946 0.5350515 0.3243927 0.3806867 0.4580532 0.5221023 0.6952034 #> [288] 0.6533267 0.4921038 0.7334513 0.5032917 0.6676040 0.7125776 0.5408955 #> [295] 0.3928002 0.3287531 0.5012287 0.3799930 0.5116441 0.6056690 0.3866806 #> [302] 0.5389672 0.6632026 0.5184388 0.5110211 0.3489454 0.5529228 0.5960493 #> [309] 0.6654337 0.4993021 0.5917869 0.6261243 0.4672141 0.3754637 0.5558017 #> [316] 0.4886690 0.6332012 0.5225871 0.4343941 0.3742135 0.4093834 0.5497649 #> [323] 0.4357071 0.4759894 0.4305244 0.7010143 0.4382944 0.4720362 0.5581313 #> [330] 0.5119253 0.6836475 0.5343088 0.6004379 0.4881342 0.4231969 0.3598382 #> [337] 0.5185657 0.5320229 0.6655590 0.3745967 0.4716455 0.5660316 0.4365982 #> [344] 0.4179904 0.4207624 0.6487914 0.5373581 0.4355690 0.4061525 0.6182296 #> [351] 0.5572310 0.4497439 0.5003683 0.7202764 0.4874284 0.4946942 0.4990330 #> [358] 0.5706586 0.4913570 0.5693478 0.6996120 0.5472478 0.4821505 0.5208240 #> [365] 0.3933438 0.4011692 0.7344256 0.6700732 0.4964954 0.7129060 0.6702195 #> [372] 0.4585901 0.6426145 0.6671890 0.4801019 0.6179605 0.5112422 0.4246498 #> [379] 0.4266410 0.5785052 0.2852388 0.4692045 0.8124865 0.4634984 0.4834419 #> [386] 0.6363941 0.5877120 0.6696918 0.5593411 0.3236144 0.4726739 0.4575294 #> [393] 0.5264682 0.5454175 0.5467156 0.6180588 0.5448598 0.5860055 0.4319808 #> [400] 0.5860403 0.5090573 0.6411664 0.4735417 0.5054485 0.5731065 0.3947812 #> [407] 0.5391848 0.5161291 0.3971941 0.4996523 0.4577200 0.4930528 0.4662944 #> [414] 0.5085744 0.4992777 0.4547934 0.4516528 0.6021815 0.8248945 0.3808702 #> [421] 0.6519603 0.6931349 0.6413948 0.4979950 0.4312137 0.4495234 0.3277589 #> [428] 0.6798028 0.5036491 0.6084424 0.4018696 0.5061163 0.5612356 0.5289293 #> [435] 0.4059210 0.4530022 0.3560057 0.5884984 0.4883997 0.3766108 0.5073782 #> [442] 0.2827512 0.6967117 0.6114084 0.5037660 0.5014076 0.5037769 0.4918257 #> [449] 0.4966520 0.5466264 0.5315695 0.4525137 0.4324339 0.4516680 0.5966435 #> [456] 0.4726886 0.6759836 0.4992247 0.3415949 0.6323136 0.5374092 0.6687381 #> [463] 0.4824828 0.3535219 0.3766057 0.3964241 0.5403719 0.5217721 0.6596528 #> [470] 0.4027495 0.4914423 0.6325325 0.4887104 0.4170075 0.4337061 0.4749667 #> [477] 0.6291675 0.4954692 0.5892452 0.5019742 0.5437775 0.4848289 0.6942602 #> [484] 0.5734855 0.6626265 0.5874782 0.5319290 0.5711304 0.5026551 0.6049298 #> [491] 0.4248816 0.6497884 0.6164713 0.5809698 0.5410465 0.6120050 0.5736361 #> [498] 0.5327275 0.5885373 0.5476825 #>  #> $ci #>     97.5%      2.5%  #> 0.2297709 0.6066792  #>"},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_subsampling.html","id":null,"dir":"Reference","previous_headings":"","what":"Uncertainty of KL divergence estimate using Politis/Romano's subsampling bootstrap. — kld_ci_subsampling","title":"Uncertainty of KL divergence estimate using Politis/Romano's subsampling bootstrap. — kld_ci_subsampling","text":"function computes confidence interval KL divergence based subsampling bootstrap Politis Romano. calculated interval asymptotic coverage \\(1 - \\alpha\\) long \\(b_n/n\\rightarrow 0\\), \\(b_n\\rightarrow\\infty\\) \\(\\frac{\\tau_b}{\\tau_n}\\rightarrow 0\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_subsampling.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Uncertainty of KL divergence estimate using Politis/Romano's subsampling bootstrap. — kld_ci_subsampling","text":"","code":"kld_ci_subsampling(   X,   Y = NULL,   q = NULL,   estimator = kld_est_nn,   B = 500L,   alpha = 0.05,   subsample.size = function(x) x^(2/3),   convergence.rate = sqrt )"},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_subsampling.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Uncertainty of KL divergence estimate using Politis/Romano's subsampling bootstrap. — kld_ci_subsampling","text":"X, Y n--d m--d matrices, representing n samples true distribution \\(P\\) m samples approximate distribution \\(Q\\), d dimensions. Vector input treated column matrix. Y can left blank q specified (see ). q density function approximate distribution \\(Q\\). Either Y q must specified. estimator Kullback-Leibler divergence estimation method; function expecting two inputs (X Y q, depending arguments provided). Defaults kld_est_nn. B Number bootstrap replicates (default: 500), larger, accurate, also computationally expensive. alpha Error level, defaults 0.05. subsample.size function specifying size subsamples, defaults \\(f(x) = x^{2/3}\\). convergence.rate function computing convergence rate estimator function sample sizes. Defaults \\(f(x) = x^{1/2}\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_subsampling.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Uncertainty of KL divergence estimate using Politis/Romano's subsampling bootstrap. — kld_ci_subsampling","text":"list fields \"est\" (estimated KL divergence), \"boot\" (length B numeric vector KL divergence estimates bootstrap subsamples), \"ci\" (length 2 vector containing lower upper limits estimated confidence interval).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_subsampling.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Uncertainty of KL divergence estimate using Politis/Romano's subsampling bootstrap. — kld_ci_subsampling","text":"Reference: Politis Romano, \"Large sample confidence regions based subsamples minimal assumptions\", Annals Statistics, Vol. 22, . 4 (1994).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_ci_subsampling.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Uncertainty of KL divergence estimate using Politis/Romano's subsampling bootstrap. — kld_ci_subsampling","text":"","code":"# 1D Gaussian (one- and two-sample problems) X <- rnorm(100) Y <- rnorm(100, mean = 1, sd = 2) q <- function(x) dnorm(x, mean =1, sd = 2) kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2) #> [1] 0.4431472 kld_est_nn(X, Y = Y) #> [1] 0.4519413 kld_est_nn(X, q = q) #> [1] 0.5336944 kld_ci_subsampling(X, Y)$ci #>       2.5%      97.5%  #> 0.02567606 0.88760396  kld_ci_subsampling(X, q = q)$ci #>      2.5%     97.5%  #> 0.2522395 0.8322964"},{"path":"https://niklhart.github.io/kldest/reference/kld_discrete.html","id":null,"dir":"Reference","previous_headings":"","what":"Analytical KL divergence for two discrete distributions — kld_discrete","title":"Analytical KL divergence for two discrete distributions — kld_discrete","text":"Analytical KL divergence two discrete distributions","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_discrete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analytical KL divergence for two discrete distributions — kld_discrete","text":"","code":"kld_discrete(P, Q)"},{"path":"https://niklhart.github.io/kldest/reference/kld_discrete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analytical KL divergence for two discrete distributions — kld_discrete","text":"P, Q Numerical arrays dimensions, representing discrete probability distributions","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_discrete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analytical KL divergence for two discrete distributions — kld_discrete","text":"scalar (Kullback-Leibler divergence)","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_discrete.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analytical KL divergence for two discrete distributions — kld_discrete","text":"","code":"# 1-D example P <- 1:4/10 Q <- rep(0.25,4) kld_discrete(P,Q) #> [1] 0.1064401  # The above example in 2-D P <- matrix(1:4/10,nrow=2) Q <- matrix(0.25,nrow=2,ncol=2) kld_discrete(P,Q) #> [1] 0.1064401"},{"path":"https://niklhart.github.io/kldest/reference/kld_est.html","id":null,"dir":"Reference","previous_headings":"","what":"Kullback-Leibler divergence estimator for discrete, continuous or mixed data. — kld_est","title":"Kullback-Leibler divergence estimator for discrete, continuous or mixed data. — kld_est","text":"two mixed continuous/discrete distributions densities \\(p\\) \\(q\\), denoting \\(x = (x_\\text{c},x_\\text{d})\\), Kullback-Leibler divergence \\(D_{KL}(p||q)\\) given $$D_{KL}(p||q) = \\sum_{x_d} \\int p(x_c,x_d) \\log\\left(\\frac{p(x_c,x_d)}{q(x_c,x_d)}\\right)dx_c.$$ Conditioning discrete variables \\(x_d\\), can re-written $$D_{KL}(p||q) = \\sum_{x_d} p(x_d) D_{KL}\\big(p(\\cdot|x_d)||q(\\cdot|x_d)\\big) + D_{KL}\\big(p_{x_d}||q_{x_d}\\big).$$ , terms $$D_{KL}\\big(p(\\cdot|x_d)||q(\\cdot|x_d)\\big)$$ approximated via nearest neighbour- kernel-based density estimates datasets X Y stratified discrete variables, $$D_{KL}\\big(p_{x_d}||q_{x_d}\\big)$$ approximated using relative frequencies.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kullback-Leibler divergence estimator for discrete, continuous or mixed data. — kld_est","text":"","code":"kld_est(   X,   Y = NULL,   q = NULL,   estimator.continuous = kld_est_nn,   estimator.discrete = kld_est_discrete,   vartype = NULL )"},{"path":"https://niklhart.github.io/kldest/reference/kld_est.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kullback-Leibler divergence estimator for discrete, continuous or mixed data. — kld_est","text":"X, Y Data frames matrices number columns d (multivariate samples), numeric/character vectors (univariate samples, .e. d=1), representing n samples true distribution \\(P\\) m samples approximate distribution \\(Q\\), d dimensions. Y can left blank q specified (see ). q density function approximate distribution \\(Q\\). Either Y q must specified. general, q must given decomposed form, \\(q(y_c|y_d)q(y_d)\\), specified named list field cond conditional density \\(q(y_c|y_d)\\) (function expects two arguments y_c y_d) disc discrete marginal density \\(q(y_d)\\) (function expects one argument y_d). possible, may preferable simulate large sample \\(Q\\) use two-sample syntax instead. compatibility continuous discrete one-sample estimators, sample(s) /continuous discrete, instead specifying q length 2 list, may also given function handle computing continous discrete density. estimator.continuous, estimator.discrete KL divergence estimators continuous discrete data, respectively. function two arguments X Y X q, depending whether two-sample one-sample problem considered. Defaults kld_est_nn kld_est_discrete, respectively. vartype length d character vector, vartype[] = \"c\" meaning -th variable continuous, vartype[] = \"d\" meaning discrete. unspecified, vartype \"c\" numeric columns \"d\" character factor columns. default work levels discrete variables encoded using numbers (e.g., 0 females 1 males) count data.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Kullback-Leibler divergence estimator for discrete, continuous or mixed data. — kld_est","text":"scalar, estimated Kullback-Leibler divergence \\(\\hat D_{KL}(P||Q)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Kullback-Leibler divergence estimator for discrete, continuous or mixed data. — kld_est","text":"","code":"# 2D example, two samples X <- data.frame(cont  = rnorm(10),                 discr = c(rep('a',4),rep('b',6))) Y <- data.frame(cont  = c(rnorm(5), rnorm(5, sd = 2)),                 discr = c(rep('a',5),rep('b',5))) kld_est(X, Y) #> [1] 0.06884532  # 2D example, one sample X <- data.frame(cont  = rnorm(10),                 discr = c(rep(0,4),rep(1,6))) q <- list(cond = function(xc,xd) dnorm(xc, mean = xd, sd = 1),           disc = function(xd) dbinom(xd, size = 1, prob = 0.5)) kld_est(X, q = q, vartype = c(\"c\",\"d\")) #> [1] 1.033703"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_brnn.html","id":null,"dir":"Reference","previous_headings":"","what":"Bias-reduced generalized k-nearest-neighbour KL divergence estimation — kld_est_brnn","title":"Bias-reduced generalized k-nearest-neighbour KL divergence estimation — kld_est_brnn","text":"bias-reduced generalized k-NN based KL divergence estimator Wang et al. (2009) specified Eq.(29).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_brnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bias-reduced generalized k-nearest-neighbour KL divergence estimation — kld_est_brnn","text":"","code":"kld_est_brnn(X, Y, max.k = 100, warn.max.k = TRUE, eps = 0)"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_brnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bias-reduced generalized k-nearest-neighbour KL divergence estimation — kld_est_brnn","text":"X, Y n--d m--d matrices, representing n samples true distribution \\(P\\) m samples approximate distribution \\(Q\\), d dimensions. Vector input treated column matrix. Y can left blank q specified (see ). max.k Maximum numbers nearest neighbours compute (default: 100). larger max.k may yield accurate KL-D estimate (see warn.max.k), always increase computational cost. warn.max.k TRUE (default), warns max.k max.k neighbours within neighbourhood \\(\\delta\\) data point(s). case, first max.k neighbours counted. consequence, max.k may required increased. eps Error bound nearest neighbour search. value eps = 0 (default) implies exact nearest neighbour search, eps > 0 approximate nearest neighbours sought, may somewhat faster high-dimensional problems.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_brnn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bias-reduced generalized k-nearest-neighbour KL divergence estimation — kld_est_brnn","text":"scalar, estimated Kullback-Leibler divergence \\(\\hat D_{KL}(P||Q)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_brnn.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bias-reduced generalized k-nearest-neighbour KL divergence estimation — kld_est_brnn","text":"Finite sample bias reduction achieved adaptive choice number nearest neighbours. Fixing number nearest neighbours upfront, done kld_est_nn(), may result different distances \\(\\rho^l_i,\\nu^k_i\\) datapoint \\(x_i\\) \\(l\\)-th nearest neighbours \\(X\\) \\(k\\)-th nearest neighbours \\(Y\\), respectively, may lead unequal biases NN density estimation, especially high-dimensional setting. overcome issue, number neighbours \\(l,k\\) chosen way render \\(\\rho^l_i,\\nu^k_i\\) comparable, taking largest possible number neighbours \\(l_i,k_i\\) smaller \\(\\delta_i:=\\max(\\rho^1_i,\\nu^1_i)\\). Since bias reduction explicitly uses samples X Y, one-sample estimation possible using method. Reference: Wang, Kulkarni Verdú, \"Divergence Estimation Multidimensional Densities Via k-Nearest-Neighbor Distances\", IEEE Transactions Information Theory, Vol. 55, . 5 (2009). DOI: https://doi.org/10.1109/TIT.2009.2016060","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_brnn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bias-reduced generalized k-nearest-neighbour KL divergence estimation — kld_est_brnn","text":"","code":"# KL-D between one or two samples from 1-D Gaussians: X <- rnorm(100) Y <- rnorm(100, mean = 1, sd = 2) q <- function(x) dnorm(x, mean = 1, sd =2) kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2) #> [1] 0.4431472 kld_est_nn(X, Y) #> [1] 0.4523245 kld_est_nn(X, q = q) #> [1] 0.4556598 kld_est_nn(X, Y, k = 5) #> [1] 0.5720222 kld_est_nn(X, q = q, k = 5) #> [1] 0.430842 kld_est_brnn(X, Y) #> [1] 0.4866684   # KL-D between two samples from 2-D Gaussians: X1 <- rnorm(100) X2 <- rnorm(100) Y1 <- rnorm(100) Y2 <- Y1 + rnorm(100) X <- cbind(X1,X2) Y <- cbind(Y1,Y2) kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),              mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2)) #> [1] 0.5 kld_est_nn(X, Y) #> [1] 0.1937211 kld_est_nn(X, Y, k = 5) #> [1] 0.2505544 kld_est_brnn(X, Y) #> [1] 0.3220004"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_discrete.html","id":null,"dir":"Reference","previous_headings":"","what":"Plug-in KL divergence estimator for samples from discrete distributions — kld_est_discrete","title":"Plug-in KL divergence estimator for samples from discrete distributions — kld_est_discrete","text":"Plug-KL divergence estimator samples discrete distributions","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_discrete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plug-in KL divergence estimator for samples from discrete distributions — kld_est_discrete","text":"","code":"kld_est_discrete(X, Y = NULL, q = NULL)"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_discrete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plug-in KL divergence estimator for samples from discrete distributions — kld_est_discrete","text":"X, Y n--d m--d matrices data frames, representing n samples true discrete distribution \\(P\\) m samples approximate discrete distribution \\(Q\\), d dimensions. Vector input treated column matrix. Argument Y can omitted argument q given (see ). q probability mass function approximate distribution \\(Q\\). Currently, one-sample problem implemented d=1.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_discrete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plug-in KL divergence estimator for samples from discrete distributions — kld_est_discrete","text":"scalar, estimated Kullback-Leibler divergence \\(\\hat D_{KL}(P||Q)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_discrete.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plug-in KL divergence estimator for samples from discrete distributions — kld_est_discrete","text":"","code":"# 1D example, two samples X <- c(rep('M',5),rep('F',5)) Y <- c(rep('M',6),rep('F',4)) kld_est_discrete(X, Y) #> [1] 0.020411  # 1D example, one sample X <- c(rep(0,4),rep(1,6)) q <- function(x) dbinom(x, size = 1, prob = 0.5) kld_est_discrete(X, q = q) #> [1] 0.02013551"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde.html","id":null,"dir":"Reference","previous_headings":"","what":"Kernel density-based Kullback-Leibler divergence estimation in any dimension — kld_est_kde","title":"Kernel density-based Kullback-Leibler divergence estimation in any dimension — kld_est_kde","text":"Disclaimer: function use binning /fast Fourier transform hence, extremely slow even moderate datasets. reason, exported currently.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Kernel density-based Kullback-Leibler divergence estimation in any dimension — kld_est_kde","text":"","code":"kld_est_kde(X, Y, hX = NULL, hY = NULL, rule = c(\"Silverman\", \"Scott\"))"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Kernel density-based Kullback-Leibler divergence estimation in any dimension — kld_est_kde","text":"X, Y n--d m--d matrices, representing n samples true distribution \\(P\\) m samples approximate distribution \\(Q\\), d dimensions. Vector input treated column matrix. hX, hY Positive scalars length d vectors, representing bandwidth parameters (possibly different component) density estimates \\(P\\) \\(Q\\), respectively. unspecified, heurestic specified via rule argument used. rule heuristic computing arguments hX /hY. default \"silverman\" Silverman's rule $$h_i = \\sigma_i\\left(\\frac{4}{(2+d)n}\\right)^{1/(d+4)}.$$ alternative, Scott's rule \"scott\" can used, $$h_i = \\frac{\\sigma_i}{n^{1/(d+4)}}.$$","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Kernel density-based Kullback-Leibler divergence estimation in any dimension — kld_est_kde","text":"scalar, estimated Kullback-Leibler divergence \\(\\hat D_{KL}(P||Q)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Kernel density-based Kullback-Leibler divergence estimation in any dimension — kld_est_kde","text":"estimation method approximates densities unknown distributions \\(P\\) \\(Q\\) kernel density estimates, using sample size- dimension-dependent bandwidth parameter Gaussian kernel. works number dimensions slow.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Kernel density-based Kullback-Leibler divergence estimation in any dimension — kld_est_kde","text":"","code":"# KL-D between two samples from 1-D Gaussians: X <- rnorm(100) Y <- rnorm(100, mean = 1, sd = 2) kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2) #> [1] 0.4431472 kld_est_kde1(X, Y) #> [1] 0.4151536 kld_est_nn(X, Y) #> [1] 0.04735289 kld_est_brnn(X, Y) #> [1] 0.1903968  # KL-D between two samples from 2-D Gaussians: X1 <- rnorm(100) X2 <- rnorm(100) Y1 <- rnorm(100) Y2 <- Y1 + rnorm(100) X <- cbind(X1,X2) Y <- cbind(Y1,Y2) kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),              mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2)) #> [1] 0.5 kld_est_kde2(X, Y) #> [1] 0.1202541 kld_est_nn(X, Y) #> [1] 0.140686 kld_est_brnn(X, Y) #> [1] 0.1379448"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde1.html","id":null,"dir":"Reference","previous_headings":"","what":"1-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde1","title":"1-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde1","text":"estimation method approximates densities unknown distributions \\(P\\) \\(Q\\) kernel density estimate using function 'density' package 'stats'. two-sample, one-sample problem implemented.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde1.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"1-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde1","text":"","code":"kld_est_kde1(X, Y, MC = FALSE, ...)"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde1.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"1-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde1","text":"X, Y Numeric vectors single-column matrices, representing samples true distribution \\(P\\) approximate distribution \\(Q\\), respectively. MC boolean: use Monte Carlo approximation instead numerical integration via trapezoidal rule (default: FALSE)? ... parameters passed stats::density (e.g., argument bw)","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde1.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"1-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde1","text":"scalar, estimated Kullback-Leibler divergence \\(\\hat D_{KL}(P||Q)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde1.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"1-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde1","text":"","code":"# KL-D between two samples from 1D Gaussians: X <- rnorm(100) Y <- rnorm(100, mean = 1, sd = 2) kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2) #> [1] 0.4431472 kld_est_kde1(X,Y) #> [1] 0.4059567 kld_est_kde1(X,Y, MC = TRUE) #> [1] 0.456112"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde2.html","id":null,"dir":"Reference","previous_headings":"","what":"2-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde2","title":"2-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde2","text":"estimation method approximates densities unknown bivariate distributions \\(P\\) \\(Q\\) kernel density estimates using function 'bkde' package 'KernSmooth'. 'KernSmooth' installed, message issued (much) slower function 'kld_est_kde' used instead.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde2.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"2-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde2","text":"","code":"kld_est_kde2(   X,   Y,   MC = FALSE,   hX = NULL,   hY = NULL,   rule = c(\"Silverman\", \"Scott\"),   eps = 1e-05 )"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde2.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"2-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde2","text":"X, Y n--2 m--2 matrices, representing n samples bivariate true distribution \\(P\\) m samples approximate distribution \\(Q\\), respectively. MC boolean: use Monte Carlo approximation instead numerical integration via trapezoidal rule (default: FALSE)? Currently, option implemented, .e. value TRUE results error. hX, hY Bandwidths kernel density estimates \\(P\\) \\(Q\\), respectively. default NULL means determined argument rule. rule heuristic derive parameters hX hY, default \"Silverman\", means $$h_i = \\sigma_i\\left(\\frac{4}{(2+d)n}\\right)^{1/(d+4)}.$$ eps nonnegative scalar; eps > 0, \\(Q\\) estimated mixture kernel density estimate uniform distribution computational grid. weight uniform component eps times maximum density estimate \\(Q\\). increases robustness estimator expense additional bias. Defaults eps = 1e-5.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde2.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"2-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde2","text":"scalar, estimated Kullback-Leibler divergence \\(\\hat D_{KL}(P||Q)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_kde2.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"2-D kernel density-based estimation of Kullback-Leibler divergence — kld_est_kde2","text":"","code":"# KL-D between two samples from 2-D Gaussians: X1 <- rnorm(1000) X2 <- rnorm(1000) Y1 <- rnorm(1000) Y2 <- Y1 + rnorm(1000) X <- cbind(X1,X2) Y <- cbind(Y1,Y2) kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),              mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2)) #> [1] 0.5 kld_est_kde2(X,Y) #> [1] 0.2809953 # kld_est_kde2(X,Y, MC = TRUE)"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_nn.html","id":null,"dir":"Reference","previous_headings":"","what":"k-nearest neighbour KL divergence estimator — kld_est_nn","title":"k-nearest neighbour KL divergence estimator — kld_est_nn","text":"function estimates Kullback-Leibler divergence \\(D_{KL}(P||Q)\\) two continuous distributions \\(P\\) \\(Q\\) using nearest-neighbour (NN) density estimation Monte Carlo approximation \\(D_{KL}(P||Q)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_nn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"k-nearest neighbour KL divergence estimator — kld_est_nn","text":"","code":"kld_est_nn(X, Y = NULL, q = NULL, k = 1L, eps = 0, log.q = FALSE)"},{"path":"https://niklhart.github.io/kldest/reference/kld_est_nn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"k-nearest neighbour KL divergence estimator — kld_est_nn","text":"X, Y n--d m--d matrices, representing n samples true distribution \\(P\\) m samples approximate distribution \\(Q\\), d dimensions. Vector input treated column matrix. Y can left blank q specified (see ). q density function approximate distribution \\(Q\\). Either Y q must specified. k number nearest neighbours consider NN density estimation. Larger values k generally increase bias, decrease variance estimator. Defaults k = 1. eps Error bound nearest neighbour search. value eps = 0 (default) implies exact nearest neighbour search, eps > 0 approximate nearest neighbours sought, may somewhat faster high-dimensional problems. log.q TRUE, function q log-density rather density approximate distribution \\(Q\\) (default: log.q = FALSE).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_nn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"k-nearest neighbour KL divergence estimator — kld_est_nn","text":"scalar, estimated Kullback-Leibler divergence \\(\\hat D_{KL}(P||Q)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_nn.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"k-nearest neighbour KL divergence estimator — kld_est_nn","text":"Input estimation sample X \\(P\\) either density function q \\(Q\\) (one-sample problem) sample Y \\(Q\\) (two-sample problem). two-sample problem, estimator Eq.(5) Wang et al. (2009). one-sample problem, asymptotic bias (expectation Gamma distribution) substracted, see Pérez-Cruz (2008), Eq.(18). References: Wang, Kulkarni Verdú, \"Divergence Estimation Multidimensional Densities Via k-Nearest-Neighbor Distances\", IEEE Transactions Information Theory, Vol. 55, . 5 (2009). Pérez-Cruz, \"Kullback-Leibler Divergence Estimation Continuous Distributions\", IEEE International Symposium Information Theory (2008).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_est_nn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"k-nearest neighbour KL divergence estimator — kld_est_nn","text":"","code":"# KL-D between one or two samples from 1-D Gaussians: X <- rnorm(100) Y <- rnorm(100, mean = 1, sd = 2) q <- function(x) dnorm(x, mean = 1, sd =2) kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2) #> [1] 0.4431472 kld_est_nn(X, Y) #> [1] 0.6304971 kld_est_nn(X, q = q) #> [1] 0.6761798 kld_est_nn(X, Y, k = 5) #> [1] 0.4888598 kld_est_nn(X, q = q, k = 5) #> [1] 0.5385464 kld_est_brnn(X, Y) #> [1] 0.4307868   # KL-D between two samples from 2-D Gaussians: X1 <- rnorm(100) X2 <- rnorm(100) Y1 <- rnorm(100) Y2 <- Y1 + rnorm(100) X <- cbind(X1,X2) Y <- cbind(Y1,Y2) kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),              mu2 = rep(0,2), sigma2 = matrix(c(1,1,1,2),nrow=2)) #> [1] 0.5 kld_est_nn(X, Y) #> [1] 0.0982812 kld_est_nn(X, Y, k = 5) #> [1] 0.01586174 kld_est_brnn(X, Y) #> [1] 0.1353306"},{"path":"https://niklhart.github.io/kldest/reference/kld_exponential.html","id":null,"dir":"Reference","previous_headings":"","what":"Analytical KL divergence for two univariate exponential distributions — kld_exponential","title":"Analytical KL divergence for two univariate exponential distributions — kld_exponential","text":"function computes \\(D_{KL}(p||q)\\), \\(p\\sim \\text{Exp}(\\lambda_1)\\) \\(q\\sim \\text{Exp}(\\lambda_2)\\), rate parametrization.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_exponential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analytical KL divergence for two univariate exponential distributions — kld_exponential","text":"","code":"kld_exponential(lambda1, lambda2)"},{"path":"https://niklhart.github.io/kldest/reference/kld_exponential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analytical KL divergence for two univariate exponential distributions — kld_exponential","text":"lambda1 scalar (rate parameter true exponential distribution) lambda2 scalar (rate parameter approximate exponential distribution)","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_exponential.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analytical KL divergence for two univariate exponential distributions — kld_exponential","text":"scalar (Kullback-Leibler divergence)","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_exponential.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analytical KL divergence for two univariate exponential distributions — kld_exponential","text":"","code":"kld_exponential(lambda1 = 1, lambda2 = 2) #> [1] 0.3068528"},{"path":"https://niklhart.github.io/kldest/reference/kld_gaussian.html","id":null,"dir":"Reference","previous_headings":"","what":"Analytical KL divergence for two uni- or multivariate Gaussian distributions — kld_gaussian","title":"Analytical KL divergence for two uni- or multivariate Gaussian distributions — kld_gaussian","text":"function computes \\(D_{KL}(p||q)\\), \\(p\\sim \\mathcal{N}(\\mu_1,\\Sigma_1)\\) \\(q\\sim \\mathcal{N}(\\mu_2,\\Sigma_2)\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_gaussian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analytical KL divergence for two uni- or multivariate Gaussian distributions — kld_gaussian","text":"","code":"kld_gaussian(mu1, sigma1, mu2, sigma2)"},{"path":"https://niklhart.github.io/kldest/reference/kld_gaussian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analytical KL divergence for two uni- or multivariate Gaussian distributions — kld_gaussian","text":"mu1 numeric vector (mean true Gaussian) sigma1 s.p.d. matrix (Covariance matrix true Gaussian) mu2 numeric vector (mean approximate Gaussian) sigma2 s.p.d. matrix  (Covariance matrix approximate Gaussian)","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_gaussian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analytical KL divergence for two uni- or multivariate Gaussian distributions — kld_gaussian","text":"scalar (Kullback-Leibler divergence)","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_gaussian.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analytical KL divergence for two uni- or multivariate Gaussian distributions — kld_gaussian","text":"","code":"kld_gaussian(mu1 = 1, sigma1 = 1, mu2 = 1, sigma2 = 2^2) #> [1] 0.3181472 kld_gaussian(mu1 = rep(0,2), sigma1 = diag(2),                 mu2 = rep(1,2), sigma2 = matrix(c(1,0.5,0.5,1), nrow = 2)) #> [1] 0.856159"},{"path":"https://niklhart.github.io/kldest/reference/kld_uniform.html","id":null,"dir":"Reference","previous_headings":"","what":"Analytical KL divergence for two uniform distributions — kld_uniform","title":"Analytical KL divergence for two uniform distributions — kld_uniform","text":"function computes \\(D_{KL}(p||q)\\), \\(p\\sim \\text{U}(a_1,b_1)\\) \\(q\\sim \\text{U}(a_2,b_2)\\), \\(a_2<a_1<b_1<b_2\\).","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_uniform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Analytical KL divergence for two uniform distributions — kld_uniform","text":"","code":"kld_uniform(a1, b1, a2, b2)"},{"path":"https://niklhart.github.io/kldest/reference/kld_uniform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Analytical KL divergence for two uniform distributions — kld_uniform","text":"a1, b1 Range true uniform distribution a2, b2 Range approximate uniform distribution","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_uniform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Analytical KL divergence for two uniform distributions — kld_uniform","text":"scalar (Kullback-Leibler divergence)","code":""},{"path":"https://niklhart.github.io/kldest/reference/kld_uniform.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Analytical KL divergence for two uniform distributions — kld_uniform","text":"","code":"kld_uniform(a1 = 0, b1 = 1, a2 = 0, b2 = 2) #> [1] 0.6931472"},{"path":"https://niklhart.github.io/kldest/reference/kldest-package.html","id":null,"dir":"Reference","previous_headings":"","what":"kldest: Sample-based estimation of Kullback-Leibler divergence — kldest-package","title":"kldest: Sample-based estimation of Kullback-Leibler divergence — kldest-package","text":"collection estimation algorithms Kullback-Leibler divergence, based one two samples, including uncertainty quantification. Distributions can uni- multivariate continous, discrete mixed.","code":""},{"path":"https://niklhart.github.io/kldest/reference/kldest-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"kldest: Sample-based estimation of Kullback-Leibler divergence — kldest-package","text":"Maintainer: Niklas Hartung niklas.hartung@gmail.com (ORCID)","code":""},{"path":"https://niklhart.github.io/kldest/reference/mvdnorm.html","id":null,"dir":"Reference","previous_headings":"","what":"Probability density function of multivariate Gaussian distribution — mvdnorm","title":"Probability density function of multivariate Gaussian distribution — mvdnorm","text":"Probability density function multivariate Gaussian distribution","code":""},{"path":"https://niklhart.github.io/kldest/reference/mvdnorm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Probability density function of multivariate Gaussian distribution — mvdnorm","text":"","code":"mvdnorm(x, mu, Sigma)"},{"path":"https://niklhart.github.io/kldest/reference/mvdnorm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Probability density function of multivariate Gaussian distribution — mvdnorm","text":"x vector length d Gaussian density evaluated. mu vector length d, mean Gaussian distribution. Sigma d--d matrix, covariance matrix Gaussian distribution.","code":""},{"path":"https://niklhart.github.io/kldest/reference/mvdnorm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Probability density function of multivariate Gaussian distribution — mvdnorm","text":"probability density \\(N(\\mu,\\Sigma)\\) evaluated x.","code":""},{"path":"https://niklhart.github.io/kldest/reference/mvdnorm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Probability density function of multivariate Gaussian distribution — mvdnorm","text":"","code":"# 1D example mvdnorm(x = 2, mu = 1, Sigma = 2) #> [1] 0.2196956 dnorm(x = 2, mean = 1, sd = sqrt(2)) #> [1] 0.2196956 # Independent 2D example mvdnorm(x = c(2,2), mu = c(1,1), Sigma = diag(1:2)) #> [1] 0.05315991 prod(dnorm(x = c(2,2), mean = c(1,1), sd = sqrt(1:2))) #> [1] 0.05315991 # Correlated 2D example mvdnorm(x = c(2,2), mu = c(1,1), Sigma = matrix(c(2,1,1,2),nrow=2)) #> [1] 0.06584074"},{"path":"https://niklhart.github.io/kldest/reference/tr.html","id":null,"dir":"Reference","previous_headings":"","what":"Matrix trace operator — tr","title":"Matrix trace operator — tr","text":"Matrix trace operator","code":""},{"path":"https://niklhart.github.io/kldest/reference/tr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Matrix trace operator — tr","text":"","code":"tr(M)"},{"path":"https://niklhart.github.io/kldest/reference/tr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matrix trace operator — tr","text":"M square matrix","code":""},{"path":"https://niklhart.github.io/kldest/reference/tr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matrix trace operator — tr","text":"matrix trace (scalar)","code":""},{"path":"https://niklhart.github.io/kldest/reference/trapz.html","id":null,"dir":"Reference","previous_headings":"","what":"Trapezoidal integration in d dimensions — trapz","title":"Trapezoidal integration in d dimensions — trapz","text":"Trapezoidal integration d dimensions","code":""},{"path":"https://niklhart.github.io/kldest/reference/trapz.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trapezoidal integration in d dimensions — trapz","text":"","code":"trapz(h, fx)"},{"path":"https://niklhart.github.io/kldest/reference/trapz.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trapezoidal integration in d dimensions — trapz","text":"h length d numeric vector grid widths. fx d-dimensional array (vector, d=1).","code":""},{"path":"https://niklhart.github.io/kldest/reference/trapz.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trapezoidal integration in d dimensions — trapz","text":"trapezoidal approximation integral.","code":""},{"path":"https://niklhart.github.io/kldest/reference/trapz.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Trapezoidal integration in d dimensions — trapz","text":"","code":"# 1D example trapz(h = 1, fx = 1:10) #> [1] 49.5 # 2D example trapz(h = c(1,1), fx = matrix(1:10, nrow = 2)) #> [1] 22"}]
