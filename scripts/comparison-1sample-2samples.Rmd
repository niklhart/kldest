---
title: "Comparison 1-sample vs 2-sample estimation"
author: "Niklas Hartung"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries}
library(kldest)
library(ggplot2)
set.seed(1)
```


## Specification of simulation scenario

### Distributions and KL-D

We investigate the following pairs of distributions,
for which analytical KL divergence values are known:

* $\mathcal{N}(0,1)$ vs. $\mathcal{N}(1,2^2)$,
* $\mathcal{U}(1,2)$ vs. $\mathcal{U}(0,4)$,
* $\text{Exp}(1)$ vs. $\text{Exp}(12)$.

```{r}
p <- list(
    gaussian    = list(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2),
    uniform     = list(a1 = 1, b1 = 2, a2 = 0, b2 = 4),
    exponential = list(lambda1 = 1, lambda2 = 1/12)
)
distributions <- list(
    gaussian = list(
        samples = function(n, m) {
            X <- rnorm(n, mean = p$gaussian$mu1, sd = sqrt(p$gaussian$sigma1))
            Y <- rnorm(m, mean = p$gaussian$mu2, sd = sqrt(p$gaussian$sigma2))
            list(X = X, Y = Y)
        },
        q = function(x) dnorm(x, mean = p$gaussian$mu2, sd = sqrt(p$gaussian$sigma2)),
        kld = do.call(kld_gaussian, p$gaussian)
    ),
    uniform = list( 
        samples = function(n, m) {
            X <- runif(n, min = p$uniform$a1, max = p$uniform$b1)
            Y <- runif(m, min = p$uniform$a2, max = p$uniform$b2)
            list(X = X, Y = Y)
        },
        q = function(x) dunif(x, min = p$uniform$a2, max = p$uniform$b2),
        kld = do.call(kld_uniform, p$uniform)
    ),
    exponential = list(
        samples = function(n, m) {
            X <- rexp(n, rate = p$exponential$lambda1)
            Y <- rexp(m, rate = p$exponential$lambda2)
            list(X = X, Y = Y)
        },
        q = function(x) dexp(x, rate = p$exponential$lambda2),
        kld = do.call(kld_exponential, p$exponential)
    )
)
```

Analytical values for Kullback-Leibler divergences in test cases:

```{r}
vapply(distributions, function(x) x$kld, 1)
```

### Simulation scenarios

For each of the distributions specified above, samples of different sizes are 
drawn, with several replicates per distribution and sample size.

```{r}
samplesize <- 10^(2:4)
nRep       <- 25L

scenarios <- combinations(
    distribution = names(distributions),
    sample.size  = samplesize,
    replicate    = 1:nRep
)
```

### Algorithms

The following algorithms are considered:

```{r}
algorithms_XY <- list(
    nn1_XY     = function(X, q) kld_est_nn(X, Y, k = 1L),
    nn2_XY     = function(X, Y) kld_est_nn(X, Y, k = 2L)
)
algorithms_Xq <- list(
    nn1_Xq = function(X, q) kld_est_nn(X, q, k = 1L),
    nn2_Xq = function(X, q) kld_est_nn(X, q, k = 2L)
)
nAlgoXY  <- length(algorithms_XY)
nAlgoXq  <- length(algorithms_Xq)
nmAlgo   <- c(names(algorithms_XY),names(algorithms_Xq))
```

## Run the simulation study 

```{r}
# allocating results matrices
nscenario  <- nrow(scenarios)
runtime <- kld <- matrix(nrow = nscenario, 
                         ncol = nAlgoXY+nAlgoXq, 
                         dimnames = list(NULL, nmAlgo))

for (i in 1:nscenario) {

    dist <- scenarios$distribution[i]
    n    <- scenarios$sample.size[i]
    
    samples <- distributions[[dist]]$sample(n = n, m = n)
    X <- samples$X
    Y <- samples$Y
    q <- distributions[[dist]]$q

    # different algorithms are evaluated on the same samples
    for (j in 1:nAlgoXY) {
        algo         <- algorithms_XY[[j]]
        start_time   <- Sys.time()
        kld[i,j]     <- algo(X, Y)
        end_time     <- Sys.time()
        runtime[i,j] <- end_time - start_time
    }
    for (j in 1:nAlgoXq) {
        nj            <- nAlgoXY+j
        algo          <- algorithms_Xq[[j]]
        start_time    <- Sys.time()
        kld[i,nj]     <- algo(X, q)
        end_time      <- Sys.time()
        runtime[i,nj] <- end_time - start_time
    }
}
```

Post-processing: combine `scenarios`, `kld` and `runtime` into a single 
data frame

```{r}
tmp1 <- cbind(scenarios, kld) |> reshape2::melt(measure.vars = nmAlgo,
                                                value.name = "kld",
                                                variable.name = "algorithm") 
tmp2 <- cbind(scenarios, runtime) |> reshape2::melt(measure.vars = nmAlgo,
                                                    value.name = "runtime",
                                                    variable.name = "algorithm") 
results <- merge(tmp1,tmp2)
rm(tmp1,tmp2)
```

Convert to factor

```{r}
results$sample.size <- as.factor(results$sample.size)
```


## Results

### Accuracy of KL divergence estimators

```{r, fig.width=7, fig.height=6}
ggplot2::ggplot(results, aes(x=sample.size, y=kld, color=algorithm)) + 
    geom_jitter(position=position_dodge(.5)) + 
    facet_wrap("distribution", scales = "free_y") +
    geom_hline(data = data.frame(distribution = names(distributions), kldtrue = vapply(distributions, function(x) x$kld,1)), 
               aes(yintercept = kldtrue))+
    xlab("Sample sizes") + ylab("KL divergence estimate") + ggtitle("Accuracy of different algorithms")

```

$\Rightarrow$ the 1-sample version has lower variance, as expected.

### Runtime of KL divergence estimators

```{r, fig.width=7, fig.height=6}
ggplot2::ggplot(results, aes(x=sample.size, y=runtime, color=algorithm)) + 
    scale_y_log10() + 
    geom_jitter(position=position_dodge(.5)) + 
    facet_wrap("distribution", scales = "free_y") +
    xlab("Sample sizes") + ylab("Runtime [sec]") + ggtitle("Runtime of different algorithms")
```

$\Rightarrow$ Runtime is slightly larger for 1-sample estimation.


## Comparison: `X` and `Y` with `m` large vs. `X` and `q`

Here, I investigate whether the two approaches have comparable performance when
the sample `Y` from the approximate distribution `q` is large.

### Gaussian model

```{r}
set.seed(0)
n <- 100
m <- 100000

nRep <- 50

Xfun <- function() rnorm(n = n, mean = 0, sd = 1)
Yfun <- function() rnorm(n = m, mean = 1, sd = 2)
q    <- function(x) dnorm(x, mean = 1, sd = 2)

kld_true <- kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2)
kld_XY <- numeric(nRep)    
kld_Xq <- numeric(nRep)    
for (i in 1:nRep) {
    X <- Xfun()
    Y <- Yfun()
    kld_XY[i] <- kld_est_nn(X, Y = Y)
    kld_Xq[i] <- kld_est_nn(X, q = q)
}
# graphics
boxplot(cbind(XY = kld_XY, Xq = kld_Xq), main = "KL divergence (Gaussian model)")
abline(h = kld_true, lty = 2)
```

$\Rightarrow$ OK!

### Uniform model

```{r}
set.seed(0)
n <- 100
m <- 100000

nRep <- 50

Xfun <- function() runif(n = n, min = 1, max = 2)
Yfun <- function() runif(n = m, min = 0, max = 4)
q    <- function(x) dunif(x, min = 0, max = 4)

kld_true <- kld_uniform(a1 = 1, b1 = 2, a2 = 0, b2 = 4)
kld_XY <- numeric(nRep)    
kld_Xq <- numeric(nRep)    
for (i in 1:nRep) {
    X <- Xfun()
    Y <- Yfun()
    kld_XY[i] <- kld_est_nn(X, Y = Y)
    kld_Xq[i] <- kld_est_nn(X, q = q)
}
# graphics
boxplot(cbind(XY = kld_XY, Xq = kld_Xq), main = "KL divergence (Uniform model)")
abline(h = kld_true, lty = 2)
```

$\Rightarrow$ OK!

### Exponential model

```{r}
set.seed(0)
n <- 100
m <- 100000

nRep <- 50

Xfun <- function() rexp(n = n, rate = 1)
Yfun <- function() rexp(n = m, rate = 1/12)
q    <- function(x) dexp(x, rate = 1/12)

kld_true <- kld_exponential(lambda1 = 1, lambda2 = 1/12)
kld_XY <- numeric(nRep)    
kld_Xq <- numeric(nRep)    
for (i in 1:nRep) {
    X <- Xfun()
    Y <- Yfun()
    kld_XY[i] <- kld_est_nn(X, Y = Y)
    kld_Xq[i] <- kld_est_nn(X, q = q)
}
# graphics
boxplot(cbind(XY = kld_XY, Xq = kld_Xq), main = "KL divergence (Exponential model)")
abline(h = kld_true, lty = 2)
```

$\Rightarrow$ OK!


## One sample k-NN variant (k>1)

### Gaussian model

```{r}
set.seed(0)
n <- 100
m <- 100000

nRep <- 50

Xfun <- function() rnorm(n = n, mean = 0, sd = 1)
Yfun <- function() rnorm(n = m, mean = 1, sd = 2)
q    <- function(x) dnorm(x, mean = 1, sd = 2)

kld_true <- kld_gaussian(mu1 = 0, sigma1 = 1, mu2 = 1, sigma2 = 2^2)
kld_XY <- numeric(nRep)    
kld_Xq <- numeric(nRep)    
for (i in 1:nRep) {
    X <- Xfun()
    Y <- Yfun()
    kld_XY[i] <- kld_est_nn(X, Y = Y, k = 2)
    kld_Xq[i] <- kld_est_nn(X, q = q, k = 2)
}
# graphics
boxplot(cbind(XY = kld_XY, Xq = kld_Xq), main = "KL divergence (Gaussian model)")
abline(h = kld_true, lty = 2)
```

$\Rightarrow$ ...



## In-depth analysis of NN-density estimation

Scenario to simulate

```{r}
n <- 100000
d <- 1
X <- rnorm(n)
p <- dnorm
```

Helper function defining the k-NN density, smoothed via a LOESS smoother,
for graphical display:

```{r}
smoothdens <- function(X, k) {
    X <- as.matrix(X)
    r_k <- RANN::nn2(X, X, k = k+1, eps = .01)$nn.dists[ ,k+1] 
    log_pX <- log(k) - log(n-1) + lgamma(0.5*d+1) - 0.5*d*log(pi) - d*log(r_k)
    log_pXl <- loess.smooth(X, log_pX)
    do.call(approxfun,log_pXl)
}
```

Compare the results for different values of `k`:

```{r}
log_p1 <- smoothdens(X, k = 1)
log_p10 <- smoothdens(X, k = 10)
```


```{r}
x <- seq(-2,2,by = 0.1)
plot(x, p(x), type = "l", ylim = c(0,0.6))
lines(x, exp(log_p1(x)), col = "red")
lines(x, exp(log_p10(x)), col = "blue")
```



