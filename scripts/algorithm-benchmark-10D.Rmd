---
title: "Algorithm performance in large dimensions: approximate vs. exact NN"
author: "Niklas Hartung"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(kldest)
set.seed(123456)
```

### Test case: 10-D correlated Gaussians

Simulation study design

```{r}
D <- 10
Sigma <- constDiagMatrix(dim = D, diag = 1, offDiag = 0.999)

```


```{r}
p <- list(
    gaussian_10d = list(
        paramTrue   = list(mu = rep(0, D), sigma = Sigma),
        paramApprox = list(mu = rep(1, D), sigma = Sigma)
    )
)
distributions <- list(
    gaussian_10d = list(
        samples = function(n, m) {
            X <- MASS::mvrnorm(n = n, 
                               mu = p$gaussian_10d$paramTrue$mu,   
                               Sigma = p$gaussian_10d$paramTrue$sigma)
            Y <- MASS::mvrnorm(n = m, 
                               mu = p$gaussian_10d$paramApprox$mu, 
                               Sigma = p$gaussian_10d$paramApprox$sigma)

            list(X = X, Y = Y)
        },
        kld = kld_gaussian(
            mu1    = p$gaussian_10d$paramTrue$mu,
            sigma1 = p$gaussian_10d$paramTrue$sigma,
            mu2    = p$gaussian_10d$paramApprox$mu,
            sigma2 = p$gaussian_10d$paramApprox$sigma)
    )
)
```

Analytical values for Kullback-Leibler divergences in test cases:

```{r}
vapply(distributions, function(x) x$kld, 1)
```

### Simulation scenarios

For each of the distributions specified above, samples of different sizes are 
drawn, with several replicates per distribution and sample size.

```{r}
samplesize <- 10^(2:4)
#n <- c(20,50,100,200,500,1000,2000,5000,10000)
nRep       <- 25L

scenarios <- combinations(
    distribution = names(distributions),
    sample.size  = samplesize,
    replicate    = 1:nRep
)
```

### Algorithms

The following algorithms are considered:

```{r}
algorithms <- list(
    nn_1_ex = function(X, Y) kld_est_nn(X = X, Y = Y, eps = 0),
    nn_1_ap = function(X, Y) kld_est_nn(X = X, Y = Y, eps = 0.1),
    nn_k_br_ex = function(X, Y) kld_est_brnn(X = X, Y = Y, warn.max.k = FALSE, eps = 0),
    nn_k_br_ap = function(X, Y) kld_est_brnn(X = X, Y = Y, warn.max.k = FALSE, eps = 0.1)
)
nAlgo   <- length(algorithms)
```

## Run the simulation study 

```{r}
# allocating results matrices
nscenario  <- nrow(scenarios)
runtime <- kldiv1d <- matrix(nrow = nscenario, 
                             ncol = nAlgo, 
                             dimnames = list(NULL, names(algorithms)))

for (i in 1:nscenario) {

    dist <- scenarios$distribution[i]
    n <- scenarios$sample.size[i]
    
    samples <- distributions[[dist]]$sample(n = n, m = n)
    X <- samples$X
    Y <- samples$Y
    
    # different algorithms are evaluated on the same samples
    for (j in 1:nAlgo) {
        algo <- algorithms[[j]]
        start_time <- Sys.time()
        kldiv1d[i,j] <- algo(X, Y)
        end_time <- Sys.time()
        runtime[i,j] <- end_time - start_time
    }
}
```

Post-processing: combine `scenarios`, `kldiv1d` and `runtime` into a single 
data frame

```{r}
tmp1 <- cbind(scenarios, kldiv1d) |> reshape2::melt(measure.vars = names(algorithms),
                                                    value.name = "kld",
                                                    variable.name = "algorithm") 
tmp2 <- cbind(scenarios, runtime) |> reshape2::melt(measure.vars = names(algorithms),
                                                    value.name = "runtime",
                                                    variable.name = "algorithm") 
results <- merge(tmp1,tmp2)
rm(tmp1,tmp2)
```

Convert to factor

```{r}
results$sample.size <- as.factor(results$sample.size)
```


## Graphical display of results

### Accuracy of KL divergence estimators

```{r, fig.width=7, fig.height=6}
ggplot2::ggplot(results, aes(x=sample.size, y=kld, color=algorithm)) + 
    geom_jitter(position=position_dodge(.5)) + 
    facet_wrap("distribution", scales = "free_y") +
    geom_hline(data = data.frame(distribution = names(distributions), kldtrue = vapply(distributions, function(x) x$kld,1)), 
               aes(yintercept = kldtrue))+
    xlab("Sample sizes") + ylab("KL divergence estimate") + ggtitle("Accuracy of different algorithms")

```

$\Rightarrow$ all estimators converge towards the true KL divergence (black solid
line). Kernel density-based estimators generally have a lower variance than 
nearest neighbour-based estimators, but show some finite sample bias, especially 
in the asymmetric exponential distribution. There is no difference between the
1-nearest neighbour and generalized k-nearest neighbour methods in terms of accuracy.

### Runtime of KL divergence estimators

```{r, fig.width=7, fig.height=6}
ggplot2::ggplot(results, aes(x=sample.size, y=runtime, color=algorithm)) + 
    scale_y_log10() + 
    geom_jitter(position=position_dodge(.5)) + 
    facet_wrap("distribution", scales = "free_y") +
    xlab("Sample sizes") + ylab("Runtime [sec]") + ggtitle("Runtime of different algorithms")
```

