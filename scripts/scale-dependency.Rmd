---
title: "Investigating scale dependency of KL divergence estimators"
author: "Niklas Hartung"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Topic

Kullback-Leibler divergence is scale-invariant, i.e. if $X\sim P$, $Y\sim Q$,
and $g(X)\sim \tilde{P}$, $g(Y)\sim \tilde{Q}$ for some transform $g$, 
then $D_{KL}(P||Q) = D_{KL}(\tilde{P},\tilde{Q})$.

The same doesn't hold for _estimators of_ Kullback-Leibler divergence, 
which  will depend on whether $X_1,...,X_n$ and $Y_1,...,Y_m$ or
$g(X_1),...,g(X_n)$ and $g(Y_1),...,g(Y_m)$ are used, i.e.
$\hat D_{KL}(\mathbf X,\mathbf Y) \neq \hat D_{KL}(g(\mathbf X),g(\mathbf Y))$.

This property can be used to our advantage: if we find a transformed scale $g$
such that $\hat D_{KL}(g(\mathbf X),g(\mathbf Y))$ has better properties than on
the original scale, we can use that estimator. 

In particular, it seems reasonable to standardise distances in the different 
variables. Here, we explore whether transforming marginals to a uniform scale 
improves estimation.
More precisely, consider a $d$-dimensional variable $x = (x_1,...,x_d)$ and
let $\hat F_i$ denote the empirical cumulative distribution function of the 
$i$-th variable in the combined sample $(\mathbf X,\mathbf Y)$.  
We then set $g(x_1,...,x_d) := (F_1(x_1),...,F_d(x_d))$ and consider the 
transformed samples $g(\mathbf X)$ and $g(\mathbf Y)$.

## Simulation study

```{r}
library(kldest)
library(ggplot2)
```

### Scenario 1: independent 2D Gaussians

We first consider a 2D problem with different locations/scales on the two 
variables, and independent variables.

True KL divergence:

```{r}
mu1 <- c(2,0); sigma1 <- c(1,100)
mu2 <- c(0,10); sigma2 <- c(1,100)
kld_true <- kld_gaussian(mu1 = mu1, sigma1 = diag(sigma1), mu2 = mu2, sigma2 = diag(sigma2))
```

```{r}
nrep <- 100L
n <- 1000L
kld_orig <- numeric(nrep)
kld_unif <- numeric(nrep)
kld_exp  <- numeric(nrep)
for (i in 1:nrep) {
    X <- cbind(rnorm(n, mean = mu1[1], sd = sqrt(sigma1[1])),
               rnorm(n, mean = mu1[2], sd = sqrt(sigma1[2])))
    Y <- cbind(rnorm(n, mean = mu2[1], sd = sqrt(sigma2[1])),
               rnorm(n, mean = mu2[2], sd = sqrt(sigma2[2])))
    X <- exp(X)
    Y <- exp(Y)
    
    kld_orig[i] <- kld_est_nn(X, Y)
    
    F1 <- ecdf(c(X[,1],Y[,1]))
    F2 <- ecdf(c(X[,2],Y[,2]))

    F12 <- function(x) cbind(F1(x[,1]),F2(x[,2]))                
    FX <- F12(X)
    FY <- F12(Y)
    kld_unif[i] <- kld_est_nn(FX, FY)
}
```

### Illustrations

Illustration of a sample on original scale

```{r}
plot(NA, 
     xlim = range(c(X[,1],Y[,1])),
     ylim = range(c(X[,2],Y[,2])),
     xlab = "First variable",
     ylab = "Second variable",
     main = "Samples on original scale")
points(X[,1], X[,2])
points(Y[,1], Y[,2], col="red2")
legend("topright", legend = c("X","Y"), col = c("black","red"), pch = 1)
```

Illustration of a sample on uniform scale

```{r}
plot(NA, 
     xlim = range(c(FX[,1],FY[,1])),
     ylim = range(c(FX[,2],FY[,2])),
     xlab = "First variable",
     ylab = "Second variable",
     main = "Samples on uniform scale")
points(FX[,1], FX[,2])
points(FY[,1], FY[,2], col="red2")
legend("topright", legend = c("X","Y"), col = c("black","red"), pch = 1)
```
### Results

Format the results

```{r}
kld <- rbind(
    data.frame(kld = kld_orig, scale = "orig"),
    data.frame(kld = kld_unif, scale = "unif")
)
```


Display the results

```{r}
ggplot(data = kld) + geom_boxplot(aes(x=scale,y=kld)) + 
    geom_hline(data = data.frame(kld = kld_true), aes(yintercept = kld), color='red')
```

$\Rightarrow$ slightly better on uniform scale. I would have expected the method 
to work much worse on the nonlinearly transformed non-uniform scale. It seems
that the NN method is quite robust to such distortions, even though it works with
Euclidian distances in the end.
